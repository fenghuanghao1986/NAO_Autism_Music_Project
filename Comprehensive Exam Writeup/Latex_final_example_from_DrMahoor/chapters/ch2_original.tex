\chapter{Related works} In this chapter we present an overview of
the previous related works. We start by reviewing and contrasting
the literature for facial features extraction form frontal 2D images
and 3D range data. Afterward, the works for 2D face recognition and
3D face recognitions are reviewed. Finally we review the presented
algorithms for multi-modal (2D + 3D) face recognition.

\section{Facial features extractions}
In both 2D and 3D face recognition systems, alignment (registration)
between the query and the template images or models is necessary
\cite{shan04}. This is the main step before the recognition step in
a typical face recognition system. This step usually has done based
on extracted facial features (i.e., fiducial points). Figure
\ref{fig:facial_features_sample} shows an example of facial features
on both a frontal and a profile face image. Beside the applications
of facial features extraction for face recognition, other
applications such as tracking, expression analysis, and animation
rely on facial features extraction. Automatic extraction of facial
features has been one of the important and challenging tasks and
many approaches and algorithms have been developed and presented for
facial features extraction. Many of these methods are cited and
reviewed in face detection and face recognition surveys
\cite{FaceRecoSurvey2003, chellappa95, yang02}.

In this Section, we review the most important algorithms and methods
for 2D and 3D facial features extraction.

\subsection{2D facial features extractions from frontal images}
Facial features extraction is defined as the process of locating
specific region, points, landmarks, or curves/contours in a given 2D
image or a 3D range image
\cite{ffe1,ffe2,ffe3,ffe4,ffe5,ffe6,ffe7,ffe8}. Although many facial
feature extraction algorithms have been proposed so far, facial
feature extraction is still a handicap to the applications due to
its high complexity. The computational cost of facial features
extraction is dominated by the estimation of the face region and the
searching for the feature points. Generic methods extract features
from images without relying on extensive knowledge about the object
of interest. They have the advantage of being typically fast and
simple. However, these approaches can become unreliable when the
quality of the image is poor or the face in the image has a
cluttered background. The algorithms for facial features extraction
can be divided into four types:
%%%
\bn

\item Generic methods based on edges, lines, and curves.

\item Template-based methods that are used to detect
facial features such as the eyes.

\item Structural matching methods that take into consideration
geometrical constraints on the features (i.e. everyone has two eyes
above the mouth).

\item Hybrid methods which combine some of the above previous
methods. \en

Table \ref{Survey_Table} reviews some of the important works
published for 2D facial features extraction from frontal images.

% I need to convert this table to Latex format!
\begin{table*}
\caption{Facial Features Extraction Methods.}
\begin{center}
\includegraphics[scale=0.6]{./chapters/figures/survey_Table.eps}
\vskip -0.1in \label{Survey_Table} \vskip -0.1in
\end{center}
\end{table*}

%%%%%%%%%%%%%
Kobayashi \etal in \cite{ffe1} described an automated algorithm for
face detection and facial features extraction from video images with
free backgrounds. The extracted features are points around the eyes,
mouth, nose and facial contours. The authors used spatiotemporal
difference images to extract these feature points. For example
blinking is used for detecting the eyes. The method proposed by De
Natalie \etal in \cite{ffe2} aimed at identifying the position of
characteristic facial elements (eyes, nose and mouth). The proposed
detection strategy is based on the identification of the face
symmetry axis, and the successive detection of eyes, mouth and other
relevant facial features using correlation.

Nikolaidis \etal in \cite{ffe3} described a method for extracting
facial features with the goal of using them in defining a sufficient
set of distances between them so that a unique description of the
structure of a face is obtained. Eyebrows, eyes, nostrils, mouth,
cheeks and chin are considered as interesting features. Candidates
for eyes, nostrils and mouth are determined by searching for minima
and maxima in the $x$ and $y$ projections of the gray level pixels
in the image. Candidates for cheeks and chin are determined by
performing adaptive hough transform on a relevant sub-image defined
according to the position of the eyes, mouth, and the ellipse
containing the main connected component of the image. In order to
acquire a more accurate model of the face, a deforming technique is
also applied to the ellipse representing the main face region.
Candidates for eyebrows are determined by adapting a proper gray
level template to an area restricted by the position of the eyes.

Lam \etal \cite{ffe9} devised an efficient approach for detecting
and locating the eyes in frontal images. Possible eye candidates in
an image are identified by means of the valley features and corners
of the eyes. Two possible eye candidates are considered to belong to
the eyes of a human face if their respective local properties are
similar; an eye window is then formed. Each of the eye region
candidates is then further verified by comparing them with a
standard eye template, and by measuring its symmetry. Yagi
\cite{ffe10} presented a system that integrates a library of 32
functions for automatic facial contour extraction. The 32 functions
can be classified into five groups such as face detection, pupil
detection, facial parts detection, facial parts contour extraction,
and face contour extraction. The system is not only useful for the
automatic 3D facial model fitting, but also for a range facial image
processing applications such as personal authentication and facial
expression analysis. Lau \etal in \cite{ffe4} proposed an energy
function which is the sum of seven weighted terms for facial
features extraction. By allocating different values for the
weighting factors, the function can extract different fiducial
points.

Yen \etal in \cite{ffe5} presented a method for facial features
extraction that uses the edge density distribution of the image. In
the preprocessing stage a face is approximated to an ellipse, and a
genetic algorithm is applied to search for the best matching region.
In the feature extraction stage, a genetic algorithm is applied to
extract the facial features, such as the eyes, nose and mouth, in
the predefined sub regions. The authors validated their method by
experimenting on various video images under natural lighting
environments and in the presence of noise and different face
orientations.

Seo \etal in \cite{ffe6} presented an active contour model based
upon color information for extracting facial features. Their
algorithm is composed of three main parts: the face region
estimation part, the detection part and the facial features
extraction part. In the face region estimation part, images are
segmented based on human skin color. In the face detection part, a
template matching method is used, and in the facial features
extraction part, an algorithm called ``color snake'' is applied to
extract facial features points within the estimated face region.

Xi \etal in \cite{ffe7} developed an algorithm for detecting human
face and extracting facial features. For this task, a flexile
coordinate system and several support vector machines were
developed. A face model for both detection and extraction was
designed based on multi-resolution wavelet decomposition (MWD). A
mean face, the MWD, and a small number of feature points were
applied to roughly detect the face. More accurate results were
achieved by a series of support vector machines. Xue \etal in
\cite{ffe11} presented a novel application of the Bayesian Shape
Model (BSM) for facial features extraction. First, a full-face model
is designed to describe the shape of a face, and the PCA is used to
estimate the shape variance of the face model. Then, the BSM is
applied to match and extract the face patch from input face images.
Finally, using the face model, the extracted face patches are easily
warped or normalized to a standard view.

Hsu \etal in \cite{Mottaleb02} proposed a face detection algorithm
from color images in the presence of varying lighting conditions as
well as complex backgrounds. Based on a novel lighting compensation
technique and a nonlinear color transformation, this method detects
skin regions over the entire image and then generates face
candidates based on the spatial arrangement of these skin patches.
The algorithm constructs eye, mouth, and boundary maps for verifying
each face candidate. Hu \etal in \cite{ffe12} proposed a facial
feature extraction method based on a linear combination model. The
model uses the knowledge of prototype faces, which are manually
labeled, to interpret novel faces. Generally, the construction of
the linear combination model depends on pixel-wise alignments of
prototypes, and the alignments are computed by an optical flow
algorithm or bootstrapping algorithm which is a full-scale
optimization and without including local information such as facial
feature points. To combine local facial features with the linear
combination model, an optical flow algorithm is proposed to compute
the pixel-wise alignments.

Gundaz \etal in \cite{ffe13} presented a method for facial features
extraction by considering the face image as a surface. Topological
properties of the facial surface, such as principal curvatures are
used to extract the eyes and mouth, which form deep valleys on the
surface. The basic idea of the proposed method is to model the
facial features as ravines on the facial surface. Ravines are points
on the surface where the maximum curvature is a local maximum in the
corresponding principal direction. Kim \etal in \cite{ffe8} proposed
an algorithm for extraction of the facial features (eyebrow, eye,
nose, and mouth) fields from gray level face images. The foundation
of this method is that eigenfeatures, derived from the eigenvalues
and eigenvectors of the gray-level data set constructed from the
feature fields, are very useful to locate these fields efficiently.
In addition, multi-resolution images, derived from a 2-D DWT
(Discrete Wavelet Transform), are used to reduce the search time for
the facial features. Nagao in \cite{ffe14} described a method for
finding the positions of features in facial images. A large class of
image variations, including those resulting from object rotation in
3D space and scaling (i.e., translation in depth), are handled. A
MAP (Maximum a Posteriori) estimation technique using Gaussian
distribution is exploited to model the relationship between images
and feature positions.

In the field of non-rigid object segmentation the so called
Deformable Models have received much attention in the recent years.
These models have proven to be efficient in many applications such
as object segmentation, appearance interpretation, motion tracking
etc. A deformable model can be characterized as a model, which under
an implicit or explicit optimization criterion deforms the shape to
match a known object in a given image. For a general review of the
most commonly used models refer to \cite{McInerney96, Jain98}.
Cootes \etal \cite{Cootes_1,Cootes_2} developed a statistical
approach called Active Shape Model (ASM) for shape modeling and
feature extraction.

In this dissertation we present an improved Active Shape Model (ASM)
for facial features extraction. The original ASM developed by Cootes
\etal \cite{Cootes_1} suffers from factors such as, poor model
initialization, intensity modeling of the local structure of the
facial features, and alignment of the shape model to a new instant
of the object in a given image using simple Euclidian
transformation. The core of our enhancement relies on three
improvements (a) initializing the ASM model using the centers of the
mouth and eyes, which are located using color information, (b)
incorporating color information to represent the local structure of
the feature points, and (c) applying 2D affine transformation in
aligning the facial features that are perturbed by head pose
variations, which effectively aligns the matched facial features to
the shape model and compensates for the effect of the head pose
variations. Experiments on a face database of 70 subjects show that
our approach outperforms the standard ASM and is successful in
extracting facial features.

\subsection{3D facial features extractions}
The previous works that utilize 3D feature information for face
recognition can be categorized into four groups:

\begin{enumerate}

\item Curvature-based methods.

\item Direct spatial surface matching.

\item Shape representation-based methods.

\item Multi-modal based combining 2D intensity with 3D range images.

\end{enumerate}
% All of the following references are from Nasserito:

Most of the early studies concentrate on curvature analysis. In
\cite{gordon91}, the surface regions from range images are
classified as convex, concave, and saddle by calculating the minimum
and maximum principal curvature. Then locations of facial features
are determined, which are used for template comparison. Lee \etal
\cite{lee90} detects corresponding regions in two range images by
graph matching based on Extended Gaussian Image (EGI). Tanaka \etal
\cite{tanaka98} also use EGI. For each face, two EGIs are
constructed from maximum principal direction and minimum principal
direction. The EGI similarity is measured by Fisher's spherical
correlation. In spatial matching approaches, recognition is via
matching facial surface directly in 3D Euclidean space
\cite{Beumier00,pan03}. In shape representation methods, the 3D
facial shape or surface is converted to another shape representation
such as point signature (PS) \cite{chua00}, spin image (SP)
\cite{Johnson99}, shape distribution \cite{Osada02}, or Local Shape
Map (LSM) \cite{wu04}. LSM is based on point signature approach and
spin images. Thus the recognition task can be achieved in the
representation domain.

Johnson \etal \cite{Johnson99} computes a spin image which describes
the shape of the surface. It is created by projecting a surface 3D
point $P$ onto 2D coordinates via a spin map function S using
oriented point on the surface. The oriented point is a coordinate
location on the surface with a normal, as in $(x,y,n)$. An image is
created by applying S to all points on the surface. This image can
be used in object recognition or 3D surface registration. Point
signature encodes a surface point p using the minimum distances of
its neighbors in a predefined periphery to a tangential plane, $P$
passing through $p$  \cite{chua00}. Neighbors of point $p$ are found
by intersecting the original surface with a sphere. $P$ is formed by
fitting a plane to the neighboring points, and by translating it to
the original point $p$. Signed distances are sampled by
$\Delta\theta$ degree intervals, thus forming a 1D parametric curve,
$d(\theta)$. In the recognition phase, each point signature
extracted from the test image is compared with each model image's
point signatures, and a total similarity between the model and scene
image is computed according to the sum of individual point signature
distances.

The facial structure can also be described with the aid of 2D or 3D
data sources. Assisted by a statistical feature location model, Lu
\etal \cite{lu05_techrep} automatically combines 3D features shape
index response, derived from the range map, with 2D intensity
corner-ness response to determine the correct positions of the
corners of the eyes and the mouth. A feature extractor based on the
directional maximum is presented to estimate the tip of the nose and
the pose angle simultaneously. In other words, the nose tip has the
largest depth value if projected onto the corrected pose direction.
The limitation of the nose extraction approach is not making full
use of the entire 3D directional rotations (rotation about the $z$
and $x$) and only yaw pose variation (rotation about the $y$-axis)
is assumed. Similarly, Wang \etal \cite{wang02} showed that by
combining 2D Gabor wavelet based image intensity features with point
signature-based 3D shape features has a superior performance than
using each modality alone.

In this dissertation, we develop an algorithm based on Gaussian
curvature to extract three feature points that is utilized in face
alignment during the recognition process. The extracted feature
points are the two inner corners of the eyes and the tip of the
nose. Before, facial extraction step from range data, we need to
find the location of the face in the range image and only keep the
face area and exclude the background, hair, and neck in the image.
Therefore, we develop a method for localizing faces in range data
using template matching. We apply normalized cross correlation to
match a facial template range image to the range images.

\section{face recognition}
%%% refernces from Nasserito:
A man-machine facial recognition system dates way back to 1965
\cite{chan65}. The authors showed that a computer program provided
with facial features extracted manually could perform recognition
with satisfactory performance. In the past few years, face
recognition has received great attention. A recent literature survey
of face recognition is given in \cite{3DFaceSurvey2006}, where most
of the paper surveys 2D algorithms. In addition, the work and survey
by Bowyer \etal \cite{bowyer04} in 2004 gives a comparison among
face recognition techniques based on 2D data, 3D data, and 2D+3D
data fusion. They reported that 3D face recognition approaches
outperform 2D approaches and the fusion of 2D + 3D data produces
slightly better results than 3D alone. A very recent survey by
Bowyer \etal \cite{3DFaceSurvey2006} in 2006 cited some approaches
that some 2D recognition approaches outperform 3D approaches. There
is a belief that it is still premature to make this judgment at this
time because current approaches did not yet make full use of 3D data
either in the recognition algorithms or the rigorous experimental
methodology.

\subsection{2D face recognition}
% from Zhao's paper: references
Many algorithms for face recognition have been proposed during the
past three decades. The literature on face recognition is vast and
diverse. Zhao \etal in \cite{FaceRecoSurvey2003} presented a
literature survey of 2D face recognition. We refer the readers to
this paper for a complete survey of the state of the art in the area
of 2D face recognition. In this Section, we review the important
approaches for 2D face recognition.

The algorithms for 2D face recognition are divided into three
categories in \cite{FaceRecoSurvey2003}. This is a clear and
high-level categorization based on a guideline suggested by the
psychological study of how humans use holistic and local features
\cite{FaceRecoSurvey2003}.

\begin{enumerate}

\item \textit{Holistic matching methods}: These methods use the whole face region
as the raw input to a recognition system. One of the most widely
used representations of the face region is eigenfaces
\cite{Eigenface1991}, which are based on principal component
analysis.

\item

\textit{Feature-based (structural) matching methods}: Typically, in
these methods, local features such as the eyes, nose, and mouth are
first extracted and their locations and local statistics (geometric
and/or appearance) are fed into a structural classifier.

\item
\textit{Hybrid methods}: Just as the human perception system uses
both local features and the whole face region to recognize a face, a
machine recognition system should use both. One can argue that these
methods could potentially offer the best of the two types of
methods.

\end{enumerate}

Table \ref{tab:2d_methods} classifies each category to sub-classes.
In the following subsections, we review in detail the most important
works for each class.

\subsection{Holistic based approaches}
\subsubsection{Principal-Component Analysis}
Successful low-dimensional reconstruction of faces using KL or PCA
projections starts from the works by Kirby and Sirovich in 1987
\cite{sirovich87} and 1990 \cite{kirby90}. Eigenpictures have been
one of the major driving forces behind face representation,
detection, and recognition. It is well known that there exist
significant statistical redundancies in natural images
\cite{ruderman94}. For a limited class of objects such as face
images that are normalized with respect to scale, translation, and
rotation, the redundancy is even greater \cite{penev96,
zhao99_thesis}. One of the best global compact representations is
Kullback-Leibler/Principal Component Analysis (KL/PCA), which
decorrelates the outputs. More specifically, sample vectors $x$ can
be expressed as linear combinations of the orthogonal basis
$\Phi_i$:

\beq x = \sum_{i=1}^{n}a_i\Phi_i \approx \sum_{i=1}^{m}a_i\Phi_i
\eeq where typically $m \ll n$. The basis $\Phi_i$ are calculated by
solving the eigenproblem:

\beq
 C\Phi = \Phi \Lambda
\eeq where $C$ is the covariance matrix for input $x$.

Such representations is less sensitive to noises that may be due to
small occlusions, as long as the topological structure does not
change. Figure \ref{fig:EigenFace} illustrates and example of
eigenface.

Turk and Pentland \cite{Eigenface1991} made a very successful
demonstration of machine recognition on faces using eigenpictures
(knows as eigenfaces) for face detection and identification. Given
the eigenfaces, every face in the database can be represented as a
vector of weights; the weights are obtained by projecting the image
into eigenface components by a simple inner product operation. When
a new test image whose identification is required is given, the new
image is also represented by its vector of weights. The
identification of the test image is done by locating the image in
the database whose weights are the closest to the weights of the
test image. By using the observation that the projection of a face
image and a non-face image are usually different, a method of
detecting the presence of a face in a given image is obtained. The
method was demonstrated using a database of 2500 face images of 16
subjects, in all combinations of three head orientations, three head
sizes, and three lighting conditions.

Moghaddam and Pentland \cite{moghaddam97} extended the standard
eigenface approach to a Bayesian approach. They used a probabilistic
measure of similarity, instead of the simple Euclidean distance used
with eigenfaces in \cite{Eigenface1991}. Practically, the major
drawback of a Bayesian method is the need to estimate probability
distributions in a high-dimensional space from very limited numbers
of training samples per class. To avoid this problem, a much simpler
two-class problem was created from the multi-class problem by using
a similarity measure based on a Bayesian analysis of image
differences. Two mutually exclusive classes were defined:
$\Omega_I$, representing intra-personal variations between multiple
images of the same individual, and $\Omega_I$, representing
extra-personal variations due to differences in identity. Assuming
that both classes are Gaussian-distributed, likelihood functions
$P(\Delta|\Omega_I)$ and $P(\Delta|\Omega_E)$ were estimated for a
given intensity difference $\Delta = I_1 - I_2$. Given these
likelihood functions and using the MAP rule, two face images are
determined to belong to the same individual if $P(\Delta|I) >
P(\Delta|E)$. A large performance improvement of this probabilistic
matching technique over standard nearest-neighbor eigenspace
matching was reported using large face data sets including the FERET
database \cite{FERET2000}. In \cite{moghaddam97}, an efficient
technique of probability density estimation was proposed by
decomposing the input space into two mutually exclusive subspaces:
the principal subspace $F$ and its orthogonal subspace $F''$ (a
similar idea was explored by Sung and Poggio \cite{Sung97}).
Covariances only in the principal subspace are estimated for use in
the Mahalanobis distance \cite{fukunaga89}. Experimental results
have been reported using different subspace dimensionalities $M_I$
and $M_E$ for $\Omega_I$ and $\Omega_E$. For example, $MI = 10$ and
$ME = 30$ were used for internal tests, while $MI = ME = 125$ were
used for the FERET test. The extra-personal eigenfaces appeared more
similar to the standard eigenfaces than the intra-personal ones, the
intra-personal eigenfaces represented subtle variations due mostly
to expression and lighting, suggesting that they are more critical
for identification \cite{moghaddam97}.

Successful face recognition systems using Linear Discriminant
Analysis/Fisher Linear Discriminant (LDA/FLD) have been reported by
\cite{Belhumeur97, Etemad97, Swets96_b, zhao98, zhao99}. LDA
training is carried out via scatter matrix analysis
\cite{fukunaga89}. For an $M-$class problem, the within- and
between-class scatter matrices $S_w$, $S_b$ are computed as follows:

\beq
 S_w = \sum_{i=1}^M Pr(\omega_i)C_i ,\\
 S_b = \sum_{i=1}^M Pr(\omega_i)(m_i - m_0)(m_i - m_0)^t,
\eeq where $Pr(\omega_i)$ is the prior class probability, and is
usually replaced by $1/M$ in practice with the assumption of equal
priors. Here $S_w$ is the within-class scatter matrix, showing the
average scatter $C_i$ of the sample vectors $x$ of different classes
$\omega_i$ around their respective means $m_i$:

\beq C_i = E[(x(\omega)- m_i)(x(\omega) - m_i)^t|\omega = \omega_i].
\eeq

Similarly, $S_b$ is the Between-class Scatter Matrix, representing
the scatter of the conditional mean vectors $m_i$ around the overall
mean vector $m_0$. A commonly used measure for quantifying
discriminatory power is the ratio of the determinant of the
between-class scatter matrix of the projected samples to the
determinant of the within-class scatter matrix:

\beq \mathcal{J}(T) = |T^t S_bT|/|T^tS_wT|. \eeq

The optimal projection matrix W which maximizes $\mathcal{J}(T)$ can
be obtained by solving a generalized eigenvalue problem:

\beq S_bW = S_wW \Lambda_W. \eeq

It is helpful to make comparisons among the so-called (linear)
projection algorithms. Here we illustrate the comparison between
eigenfaces and Fisherfaces. Similar comparisons can be made for
other methods, for example, ICA projection methods. In all these
projection algorithms, classification is performed by (1) projecting
the input x into a subspace via a projection/basis matrix $P_{roj}$:

\beq z = P_{roj}x \eeq

(2) comparing the projection coefficient vector $z$ of the input to
all the prestored projection vectors of labeled classes to determine
the input class label. The vector comparison varies in different
implementations and can influence the system’s performance
dramatically \cite{Moon01}. For example, PCA algorithms can use
either the angle or the Euclidean distance (weighted or unweighted)
between two projection vectors. For LDA algorithms, the distance can
be unweighted or weighted.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Needs work:
Swets and Weng \cite{Swets96_b} applied discriminant analysis of
eigenfeatures in an image retrieval system to determine not only
class (human face vs. nonface objects) but also individuals within
the face class. The eigenspace and LDA projections are recursively
applied to smaller and smaller sets of samples using tree-structure
learning. Such recursive partitioning is carried out for every node
until the samples assigned to the node belong to a single class. A
set of 800 images was used for training of the system based on this
approach; the training set came from 42 classes, of which human
faces belong to a single class. Testing results on images not in the
training set were 91\% for 78 face images and 87\% for 38 nonface
images based on the top choice.

A comparative performance analysis was carried out by Belhumeur
\etal \cite{Belhumeur97}. They compared four methods: (1) a
correlation-based method, (2) a variant of the linear subspace
method suggested in \cite{Shashua94}, (3) an eigenface method by
Turk and Pentland \cite{Eigenface1991}, and (4) a Fisherface method
which uses subspace projection prior to LDA projection to avoid the
possible singularity in $S_w$ as in \cite{Swets96_b}. Experiments
were performed on a database of 500 images created by
\cite{Hallianan94} and a database of 176 images created at Yale. The
results of the experiments showed that the Fisherface method
performed significantly better than the other three methods.
However, no claim was made about the relative performance of these
algorithms on larger databases.

To improve the performance of LDA-based systems, a regularized
subspace LDA system that unifies PCA and LDA was proposed in
\cite{zhao99_thesis}. They demonstrated good generalization ability
of this system by the experiments that carried out testing on new
classes/individuals without retraining the PCA bases $\Phi$, and
sometimes the LDA bases $W$. While the reason for not retraining PCA
is obvious, it is interesting to test the adaptive capability of the
system by fixing the LDA bases when images from new classes are
added. The fixed PCA subspace of dimensionality 300 was trained from
a large number of samples. An augmented set of 4056 mostly
frontal-view images constructed from the original 1078 FERET images
of 444 individuals by adding noise and mirroring was used in
\cite{zhao98}. The following three characteristics separates this
system from other LDA-based systems: (1) the unique selection of the
universal face subspace dimension, (2) the use of a weighted
distance measure, and (3) a regularized procedure that modifies the
within-class scatter matrix $S_w$. The authors selected the
dimensionality of the universal face subspace based on the
characteristics of the eigenvectors (face-like or not) instead of
the eigenvalues \cite{zhao98}, as is commonly done. Later it was
concluded in \cite{penev00} that the global face subspace
dimensionality is on the order of 400 for large databases of 5,000
images. A weighted distance metric in the projection space $z$ was
used to improve performance \cite{zhao99_thesis}. Finally, the LDA
training was regularized by modifying the $S_w$ matrix to $S_w +
\delta I$, where $\delta$ is a relatively small positive number.
Doing this solves a numerical problem when $S_w$ is close to being
singular. In the extreme case where only one sample per class is
available, this regularization transforms the LDA problem into a
standard PCA problem with $S_b$ being the covariance matrix $C$.
Applying this approach, without retraining the LDA basis, to a
testing/probe set of 46 individuals of which 24 were trained and 22
were not trained (a total of 115 images including 19 untrained
images of nonfrontal views), the authors reported the following
performance based on a front-view-only gallery database of 738
images: 85.2\% for all images and 95.1\% for frontal views.

Bartlett \etal \cite{Bartlett98} presented an argument that for
tasks such as face recognition, much of the important information is
contained in high-order statistics. So, they proposed to use
independent component analysis (ICA) to extract features for face
recognition. ICA is a generalization of principal component
analysis, which decorrelates the high-order moments of the input in
addition to the second-order moments.

Bell and Sejnowski  presented two architectures for face
recognition: the first is to find a set of statistically independent
source images that can be viewed as independent image features for a
given set of training images \cite{Bell95}, and the second is to
find image filters that produce statistically independent outputs (a
factorial code method) \cite{Bell97}. In both architectures, PCA is
used first to reduce the dimensionality of the original image size
(60 × 50). ICA is performed on the first 200 eigenvectors in the
first architecture, and is carried out on the first 200 PCA
projection coefficients in the second architecture. The authors
reported performance improvement of both architectures over
eigenfaces in the following scenario: a FERET subset consisting of
425 individuals was used; all the frontal views (one per class) were
used for training and the remaining (up to three) frontal views for
testing. Basis images of the two architectures are shown in Figure 8
along with the corresponding eigenfaces.

\subsubsection{Neural Network based approaches}
Beside the popular PCA representation and its derivatives such as
ICA and EP, other features have also been used, such as raw
intensities and edges. A fully automatic face detection/recognition
system based on a neural network is reported in \cite{Lin97}. The
proposed system is based on a probabilistic decision-based neural
network (PDBNN, an extended (DBNN) \cite{Kung95}) which consists of
three modules: a face detector, an eye localizer, and a face
recognizer. Unlike most methods, the facial regions contain the
eyebrows, eyes, and nose, but not the mouth. The rationale of using
only the upper face is to build a robust system that excludes the
influence of facial variations due to expressions that cause motion
around the mouth. To improve robustness, the segmented facial region
images are first processed to produce two features at a reduced
resolution of $14×10$: normalized intensity features and edge
features, both in the range $[0, 1]$. These features are fed into
two PDBNNs and the final recognition result is the fusion of the
outputs of these two PDBNNs.

Compared to most multi-class recognition systems that use a
discrimination function between any two classes, PDBNN has a lower
false acceptance/rejection rate because it uses the full density
description for each class. In addition, this architecture is
beneficial for hardware implementation such as distributed
computing. However, it is not clear how to accurately estimate the
full density functions for the classes when there are only limited
numbers of samples. Further, the system could have problems when the
number of classes grows exponentially.

\subsection{Geometric based approaches}
Many methods in the geometric matching category have been proposed,
including early methods based on geometry of local features
\cite{Kanade77,Kelly70} as well as 1D \cite{Samaria94_a} and
pseudo-2D \cite{Samaria94_b} HMM methods. One of the most successful
of these systems is the Elastic Bunch Graph Matching (EBGM) system
\cite{Okada98, Wiskott97}, which is based on Dynamic Link
Architecture (DLA) \cite{Buhmann90, Lades93}. Gabor-Wavelets play a
building block role for facial representation in these graph
matching methods. A typical local feature representation consists of
wavelet coefficients for different scales and rotations based on
fixed wavelet bases (called jets in \cite{Okada98}). These locally
estimated wavelet coefficients are robust to illumination change,
translation, distortion, rotation, and scaling. The basic 2D Gabor
function and its Fourier transform are

\beq g(x, y : u_0, v_0) = exp(-[x^2/2\sigma_x^2 + y^2/2\sigma_y^2]+ 2\pi i [u_0x + v_0y]), \\

G(u, v) = exp(-2\pi^2(\sigma_x^2(u - u_0)^2 + \sigma_y^2(v -
v_0)^2)),\eeq

where $\sigma_x$ and $\sigma_y$ represent the spatial widths of the
Gaussian and $(u_0, v_0)$ is the frequency of the complex sinusoid.

The DLA architecture was recently extended to Elastic Bunch Graph
Matching \cite{Wiskott97} (Figure ???). This is similar to the graph
described above, but instead of attaching only a single jet to each
node, the authors attached a set of jets (called the bunch graph
representation, Figure 10(b)), each derived from a different face
image. To handle the pose variation problem, the pose of the face is
first determined using prior class information \cite{Kruger97}, and
the ``jet'' transformations under pose variation are learned
\cite{Maurer96_a}. Systems based on the EBGM approach have been
applied to face detection and extraction, pose estimation, gender
classification, sketch-image-based recognition, and general object
recognition. The success of the EBGM system may be due to its
resemblance to the human visual system \cite{Biederman98}.

\subsubsection{Hybrid Approaches}
Hybrid approaches use both holistic and local features. For example,
the presented modular eigenfaces approach by Pentland \etal in
\cite{pentland94} uses both global eigenfaces and local
eigenfeatures. They extended the capabilities of the earlier system
\cite{Eigenface1991} in several directions. In mugshot applications,
usually a frontal and a side view of a person are available; in some
other applications, more than two views may be appropriate. One can
take two approaches to handle images from multiple views. The first
approach pools all the images and constructs a set of eigenfaces
that represent all the images from all the views. The other approach
uses separate eigenspaces for different views, so that the
collection of images taken from each view has its own eigenspace.
The second approach, known as view-based eigenspaces, performs
better.

The concept of eigenfaces can be extended to eigenfeatures, such as
eigeneyes, eigenmouth, etc. Using a limited set of images (45
persons, two views per person, with different facial expressions
such as neutral vs. smiling), recognition performance as a function
of the number of eigenvectors was measured for eigenfaces only and
for the combined representation. For lower-order spaces, the
eigenfeatures performed better than Fig. 11. Comparison of matching:
(a) test views, (b) eigenface matches, (c) eigenfeature matches
\cite{pentland94}. the eigenface \cite{pentland94}; when the
combined set was used, only marginal improvement was obtained. These
experiments support the claim that feature-based mechanisms may be
useful when gross variations are present in the input images (Figure
11).

It has been argued that practical systems should use a hybrid of PCA
and LFA. Such view has been long held in the psychology community
\cite{bruce88}. It seems to be better to estimate
eigenmodes/eigenfaces that have large eigenvalues (and so are more
robust against noise), while for estimating higher-order eigenmodes
it is better to use LFA. To support this point, it was argued in
\cite{penev96} that the leading eigenpictures are global,
integrating, or smoothing filters that are efficient in suppressing
noise, while the higher-order modes are ripply or differentiating
filters that are likely to amplify noise.

A flexible appearance model based method for automatic face
recognition was presented in \cite{lanitis95}. To identify a face,
both shape and gray-level information are modeled and used. The
shape model is an ASM; these are statistical models of the shapes of
objects which iteratively deform to fit to an example of the shape
in a new image. The statistical shape model is trained on example
images using PCA, where the variables are the coordinates of the
shape model points. For the purpose of classification, the shape
variations due to interclass variation are separated from those due
to within-class variations (such as small variations in 3D
orientation and facial expression) using discriminant analysis.
Based on the average shape of the shape model, a global shape-free
gray level model can be constructed, again using PCA. To further
enhance the robustness of the system against changes in local
appearance such as occlusions, local gray-level models are also
built on the shape model points. Simple local profiles perpendicular
to the shape boundary are used. Finally, for an input image, all
three types of information, including extracted shape parameters,
shape-free image parameters, and local profiles, are used to compute
a Mahalanobis distance for classification as illustrated in Figure
13. Based on training 10 and testing 13 images for each of 30
individuals, the classification rate was 92\% for the 10 normal
testing images and 48\% for the three difficult images.

\subsection{3D Face Recognition}
%%% The following refrences are all taken from the Proposal

3D based approaches provide a better solution to deal with
variations in pose and illuminationý\cite{3DFaceSurvey2006}. Work on
3D face recognition based on range image started in late 80's and
has grown significantly in the last few years. It is difficult to
describe all the approaches reported in the literature
ý\cite{nasser05, chang05, pan05, Passalis05, russ05, lu05, atick97,
Maurer05,ýLee03,ýYang02,ýzhao00,ýGeorghiades01,ýchua00, Bronstein05,
Hsu01,ýLee95}, so we focus only on the most important works.

Chang \etal \cite{chang05} describe a ``multi-region'' approach to
3D face recognition. It is a type of classifier ensemble approach in
which multiple overlapping sub-regions around the nose are
independently matched using ICP, and the results of the multiple 3D
matches are fused. Their experimental evaluation is based on the
FRGC version 2 data set, representing over 4,000 images from over
400 persons. With one neutral-expression image enrolled as the
gallery for each person and all subsequent images (of varied facial
expressions) used as probes, performance of 92\% rank-one
recognition is reported.

Lee \etal \cite{Lee05} propose an approach to 3D face recognition
based on the curvature values at eight feature points on the face.
Using a support vector machine for classification, they report a
rank-one recognition rate of 96\% for a data set representing 100
persons. They use a Cyberware sensor to acquire the enrollment
images and a Genex sensor to acquire the probe images. The
recognition results are called ``simulation'' results, apparently
because the feature points are manually located.

Russ \etal \cite{russ05} developed an approach using Hausdorff
distance matching on the range image representation of the 3D face.
An iterative registration procedure similar to that in ICP is used
to adjust the alignment of probe data to gallery data. Various means
of reducing space and time complexity of the matching process are
explored. Experimental results are presented on a part of the FRGC
version 1 data set, using one probe per person rather than all
available probes. Performance as high as 98.5\% rank-one
recognition, or 93.5\% verification at a false accept rate of 0.1\%,
is achieved.

Medioni \etal \cite{Medioni_03} present an authentication system
that acquires the 3D images of the subject using stereo images based
on internally and externally calibrated cameras. They use ICP
algorithm to define similarity between two faces. Their system
differs from our proposed research in a number of ways: 1) They
capture two parallel frontal images and do not use a profile view
image. 2) The resulting 3D model from their approach has only dense
depth information, while in our case we have the depth information
incorporated with a 3D generic face model that has labels for the
vertices. 3) They use ICP for matching because they do not have
correspondences, while in our case correspondence follows from the
labels of the vertices. As a result matching in our case is faster
and more accurate.

Lu and Jain ý\cite{lu05,lu06} extend previous work using an
ICP-based recognition approach ý\cite{lu04} to deal explicitly with
variations in facial expression. The problem is approached as a
rigid transformation of probe to gallery, done with ICP, along with
a non-rigid deformation, done using thin-plate spline (TPS)
technique. The approach is evaluated using a 100-person dataset,
with neutral-expression and smiling probes, matched to
neutral-expression gallery images. The gallery entries are
whole-head data structures, whereas the probes are frontal views.
Most errors after the rigid transformation result from smiling
probes, and these errors are reduced substantially after the
non-rigid deformation stage. For a total 196 probes (98 neutral and
98 smiling), performance reaches 89\% for shape-based matching and
91\% for multi-modal 3D+2D matching \cite{lu05_2}.

Pan \etal. \cite{pan05} apply PCA matching to a novel mapping of the
3D data to a range image. Finding the nose tip to use as a center
point, and an axis of symmetry to use for alignment, the face data
are mapped to a circular range image. Experimental results are
reported using the FRGC version 1 data set with 95\% rank-one
recognition rate or 2.8\% EER in a verification scenario. Chowdhury
and Chellappa \cite{Chowdhury03} obtain 3D range data of the face
from a 3D acquisition system which contains a digital camera and a
light projector that projects parallel stripes of light on the face.
The camera captures the image with light stripes deformed on the
surface of the face. They use the deformation of the stripes along
with their location on the image to estimate depth. As reported by
the authors, this method can lead to inaccurate results near the
mouth and eyes regions. Once the 3D geometry and the facial texture
are available, images can be generated for that face from almost any
pose and under any illumination using computer-graphics methods
ý[6]. For example, in [6], morphable models are utilized to map one
view of the face to another. Given a face view, 3D shape and texture
from the 3D morphable model are generated to match that image and
then using graphic approaches, it generates the other views for
recognition.

Lengagne \etal [15] proposed a 3D face reconstruction scheme using a
pair of stereo images for recognition and modeling. However, they
did not implement the recognition module. Atick \etal \cite{atick97}
proposed a reconstruction method of 3D face surfaces based on the
Karhonen­ Loeve (KL) transform and shape­from­shading. They
discussed the possibility of using eigenhead surfaces in face
recognition applications. Yan \etal. ý[20] proposed a 3D
reconstruction method to improve the performance of face recognition
by making Atick \etal.'s reconstruction method rotation-invariant.
Ben­Arie \etal. ý[38] proposed a volumetric frequency representation
(VFR) for pose invariant face recognition.

Zhao \etal. [22] proposed a method to adapt a 3D model from a
generic range map to the shape obtained from shading for enhancing
face recognition performance in different lighting and viewing
conditions. Georghiades \etal. \cite{Georghiades01} modeled
variations in illumination and pose using illumination cone models.
Zhang \etal. ý[23]ý[29] utilized a 3D generic head model to estimate
the head pose in face images, and then made use of Euclidean
distance transform to compute feature curves extracted in the image
with those in the projected head model. Their algorithm depends on
manual selection of certain feature points in the face image.

Chowdhury and Chellappa \cite{Chowdhury03} built the 3D model of the
face from monocular video sequences using structure from motion. The
accuracy of this method depends on the quality of the video
sequences. Analysis-by-synthesis was used for building 3D face
models from a single image of the face ý[6],ý[7], with excellent
results. The method in ý[7] uses 200 pre-scanned face models to
build an Eigen vector space so that an image of a face can be
represented as a projection of a linear combination of the Eigen
models. This method requires labeling seven to eight facial features
points manually for images used in building the database and for
probe images. This method takes a personal computer several minutes
to obtain the morphing parameters of the face from an image. In
ý[23] a method for building 3D models by morphing a generic model
based on different views of a face was presented. In this method
manual intervention is needed to select a point on each facial
feature component such as the eye, brow, nose, and chin. Also, their
use of texture synthesis would not work for different lighting
conditions.

\subsection{multi-modal face recognition}
% references from Nasser's proposal

In biometrics, the classical definition of “multimodal” refers to
using more than one modality or sensors, in order to increase the
accuracy and robustness of the biometric recognition system. The
goal is to counterbalance the imperfection of one modality by the
remaining ones. The fusion of modalities can take place at different
levels, namely the input sensor level, the feature level, and/or the
decision level. An example of fusion at the feature level is when
features such as those of the face and hands are combined and
represented in one feature vector. The most generally applicable
strategies are the fusion on the decision level. Each algorithm for
a single biometric modality is regarded as a single classifier that
represents the decision or matching score of the algorithm. The
details of the fusion are primarily based on the distributions of
the classifiers’ outputs. As long as the features from the different
modalities are statistically independent, the fusion of the
classifiers’ outputs seems to be the most promising approach. An
example for independent biometric modalities might be face and
fingerprint recognition, where it cannot be assumed that the
structure of the finger print contains information about the face
and vice versa. However, in the fusion of 2D and 3D face
recognition, the situation is different as both modalities are
likely to be dependent to a certain degree. For instance, the
relative positions of the eyes, nose, and mouth will be the same in
both the 2D and 3D representation of the face. Even more, there may
occur effects during acquisition like pose or occlusions that will
affect both modalities. The disadvantage of this dependency is that
the fusion at the level of the classifiers’ outputs will be less
beneficial than in the case of completely independent classifiers’
outputs. However, the great benefit of the dependency is that it
enables additional levels of fusion during the enrollment, where the


information from one modality might support the other. For example,
if both texture and shape information of each face are acquired
using 3D scanners, the image from the texture modality is
automatically registered to the shape modality. Thus it is
sufficient to find landmarks in one of the two modalities where
certain landmarks might be easier to find with higher accuracy in
one modality than the other. Often times in the research literature,
the performance of face recognition or authentication is represented
by the Cumulative Match Characteristic Curve, (CMC). The CMC curve
summarizes the percentage of a set of probes that is considered to
be correctly matched as a function of the match rank that is counted
as a correct match. The rank-one recognition rate is the most
commonly stated single number from the CMC curve. The Receiving
Operating Characteristic (ROC) summarizes the percentage of a set of
probes that is falsely rejected as a tradeoff against the percent
that is falsely accepted. The Equal Error Rate (EER), the point
where the false reject rate equals the false accept rate, is the
most commonly stated single number from the ROC curve. We list in
table 2.1 an updated version of approaches, than the one in [109],
for face recognition algorithms that combine the use of 3D and 2D
modalities. The table gives the first author and year of publication
along with number of persons in the dataset, number of images in the
dataset, image size, type of 3D data, core matching algorithm, and
reported performance.

A sparse depth map is constructed from three stereo images using
Iso-luminance lines for the stereo matching. A 3D model is
constructed from 3D geometrical features representing arcs and line
edges which are extracted from the 3D data. By searching for arcs
whose radii are of certain ranges, they first locate the irises and
the mouth and then use this information to estimate the pose. 3D
Recognition is performed by calculating the mean differences in
depth between corresponding data points in the test 3D model and all
the models in the database. Using a dataset of 10 persons, they
reported 87-96\% recognition rate. One drawback of this approach is
that only depth is used for recognition.

Bueumier \etal \cite{} use the fusion of 3D facial surfaces obtained
by using structured light and grey level clues obtained from 2D
images. When parallel structured stripes of light are projected on
the face, the parallel lines are deformed. The deformations of the
lines along with their locations in the image, captured by a camera,
are used estimate depth. The 2D and 3D data of the face are each
represented with a central profile and a lateral profile. Therefore,
they have a total of four classifiers, and an overall decision is
made using a weighted sum of 3D and 2D similarity measures. For
experiments on a subset of the data of 100 persons, using a
27-person gallery and a 29-person probe set, they reported an EER as
low as 1.4\% for recognition that merges multiple probe images per
subject.



Wang \etal \cite{wang02} present a feature-based face recognition
system based on both 3D range data as well as 2D gray scale facial
images. Feature points are extracted and described by Gabor filter
responses in the 2D domain and Point Signatures in the 3D domain.
The corresponding normalized shape and texture weight vectors are
then integrated to form an augmented vector which is used to
represent each facial image. For a given test facial image, the best
match in the model library is identified according to a similarity
function. Experimental results involving 50 persons, with six images
per person, demonstrated a recognition rate of 90\%.

Bronstein \etal \cite{Bronstein05} use range images with texture and
presented a 3D face recognition approach based on geometric
invariants, which maps 2D facial texture images into special images
that incorporate the 3D geometry of the face. Although they never
report any quantitative performance, they claim that their system
provides high recognition and can cope with variations caused by
facial expressions. Tsalakanidou \etal [115] evaluate three
different approaches, color, depth, and fusion of color and depth;
for face recognition. Experimental results, on a dataset of 40
persons, are reported for color images alone, 3D alone, and 3D plus
color. Using the Principle

Component Analysis approach (PCA), recognition rate is as high as
99\% for the multi-modal part which is found to be higher than
either 2D or 3D data alone. Similarly, using a PCA-based recognition
approach, Chang \etal [114] perform experiments with 3D and 2D
images for 200 subjects and report approximately 99\% rank-one
recognition for multi-modal fusion of 3D with 2D, 94\% for 3D alone,
and 89\% for 2D alone. The multi-modal results are obtained using a
weighted sum of the distances from individual 3D and 2D faces. Godil
\etal [116] consider fusion at image level and score level using
shape from range images and texture map information of 200 subjects
from CAESAR anthropometric database. The image level fusion is
created by concatenation of 3D shape and color map information. The
score level fusion combines scores using min, max, mean, and product
rule. They also use PCA for matching both the 2D and the 3D data.
Using the multiple score level, the reported performance is as high
as 82\%.

Papatheodorou \etal [117] use a commercial stereo camera system for
3D data acquisition. The stereo system is made up of three video
cameras and a speckled pattern projector. The projector projects a
random light pattern of dots on the surface of the face, used to
establish correspondences between two of the three cameras, allowing
the retrieval of depth information. The third camera captures the
texture information and uses a filter to eliminate the speckled
pattern projected onto the face. Their approach combines modality at
the sensor stage in 4D space as in (x, y, z, intensity). Recognition
experiments, for 62 persons, based on Iterative Closest Point (ICP), show 98-100%

recognition rate from the frontal view probes, 73-94\% for probes
with varying poses, and 69-89\% for probes with smiling expressions.
In a multimodal recognition approach integrating intensity and 3D
range data, Tsalakanidou \etal [118] use embedded hidden Markov
model technique applied to depth map. Their experimental data set
represents a small number of different persons, but each has 12
images representing various facial poses and expressions. Unlike the
majority of the reported research in the literature, they report a
higher EER for 3D than for 2D in matching frontal neutral-expression
probes to frontal neutral-expression gallery. They also report that
depth data mainly suffers from pose variations. In [119], they
describe an approach to multimodal recognition using 3D hierarchical
graph matching. Fusion of the results from the two modalities is
done at the score level. They also reported that the performance of
2D alone is only slightly less than multimodal performance and that
performance of 3D alone is substantially less than that of 2D alone.
Multimodal performance, on the FRGC version 2 data sets, is reported as 93%
verification at 0.01 False Acceptance Rate (FAR).


Lu \etal [120] report better performance with 3D matching alone than
with 2D matching alone. They also report 98\% rank-one recognition
for 3D + 2D recognition on neutral expressions and 91\% on the
larger set of neutral and smiling expressions. In their algorithm,
they use ICP style matching of 3D shape [121] to create a 3D + 2D
multimodal system and a linear discriminant analysis approach for
the 2D matching component. Their experimental data set consists of
multiple scans of each of 100 persons. Five scans with a Minolta
Vivid 910 system are taken in order to create a more accurate 3D
face model for enrolling a person. Enrollment is done with neutral
expression. An individual probe for testing uses six scans for each
person; three with neutral expression and three with smiling
expression.

Husken \etal \cite{Husken_05} describe the Viisage approach to
multi-modal recognition. The 3D matching follows the style of
hierarchical graph matching already used in Viisage's 2D face
recognition technology. This approach allows greater speed of
matching in comparison to techniques based on ICP or similar
iterative techniques. Fusion of the results from the two modalities
is done at the score level. Multi-modal performance on the FRGC
version 2 data is reported as 93\% verification at 0.01 FAR. In
addition, it is reported that performance of 2D alone is only
slightly less than multi-modal performance, and that performance of
3D alone is substantially less than that of 2D alone. In this
context, it is interesting to note that results from the Geometrix
group that originally focused on 3D face recognition show that 3D
alone outperforms 2D alone, whereas results from the Viisage group
that originally focused on 2D alone show that 2D alone outperforms
3D alone.

Maurer \etal \cite{Maurer05} describe the approach used by Geometrix
that is based on multi-modal (3D+2D) face recognition. The 3D
matching builds on the approach described by Medioni and Waupotitsch
\cite{Medioni_03}, whereas the 2D matching uses the approach of
Neven Vision ý\cite{neven}. A weighted sum rule is used to fuse the
two results, with the exception that, when the shape score (3D) is
very high, the texture score (2D) is ignored ý\cite{Maurer05}.
Experimental results on the FRGC version two data set (all versus
all matching of the 4,007 images) show 87\% verification at 0.01
FAR. They also report that the performance of 3D+2D outperforms 3D
alone by a noticeable amount, and that the verification rates for 2D
alone are below those for 3D alone.


%\subsection{Summary and Discussion}
%\subsubsection{Status of Face Recognition}
%
%
%
%\subsubsection{Lessons, Facts, and Highlights}
