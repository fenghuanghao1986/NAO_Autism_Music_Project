\chapter{Introduction}

%Undoubtedly, the role of the vision in the human life is important. Through vision, we derive a
%rich understanding of our environment. Since we obtain this understanding immediately and
%effortlessly, we can be deceived into thinking that vision should therefore be a fairly simple task
%to perform.

For the past several decades, physiologist and psycho-physicists have been striving to understand
the underlying mechanisms of human vision. On a parallel track, machine vision researchers have
been working on the development of computer systems for performing various visual tasks
automatically. Computational studies have contributed how difficult vision is to carry out, and how
complex are the processes needed to perform visual tasks successfully. This thesis is concerned
with the application of computer vision methodologies for mapping, navigation, and positioning from
flyover imagery. In particular, we are interested in the operation of submersible mobile
platforms/vehicles near the sea floor. Unlike most other applications of mobile robots which
involves a reduced set of motion components (typically 2 or 3) underwater platforms move with six
degrees of freedom and are constantly exposed to undersea currents and disturbances. This makes the
challenges to be overcome much more severe than those encountered in terrestrial and space robotics
applications. As a result, the methodologies proposed in this investigation generally address the
needs of similar flyover platforms for other environments.

%Furthermore, in most deep ocean applications, the platform carries artificial light sources for
%scene illumination. This leads to complex artifacts to be dealt in the visual data.

There is a critical need of an enabling technology for accurate mapping of benthic habitats.
Selected desired characteristics are construction of high-resolution 2-D/3-D maps and enhancing
data acquisition efficiency, including the automation of platform navigation and positioning.
Various vision systems have been developed as an aid for automated navigation and mapping in the
deployment of autonomous mobile platforms. Most vehicles rely on images from standard CCD cameras
with a single optical center and limited field of view. This has been shown to suffer from numerous
limitations \cite{adi89,win00,yag90,yag91}. Panoramic views has been proposed to overcome some of
these limitations. Panoramic imaging can play a critical role in number of applications,
capabilities, and operational modes that arise in the deployment of airborne systems, space robotic
systems, remotely operated vehicles (ROVs) and autonomous underwater vehicles (AUVs)
\cite{fir03-1,neg01}. Panoramic imaging systems have been developed that utilize a single panning
camera, fish-eye lens, or mirror(s) carefully shaped and positioned with respect to a camera
\cite{ple03,pri01,zhu99}. Panning systems have their limitations due to the required precise
mechanical and electronic scanning mechanisms, while the latter ones suffer from limited resolution
as the entire scene projects onto a single CCD sensor. Alternatively, a multi-camera configuration
can exploit the higher resolution offered by several cameras (e.g. \cite{fir03-1,yok98}). The
nature of applications of interest to us-- autonomous mapping and navigation from flyover imagery--
demands use of cameras in a down-look configuration. In comparison to an out-look panning camera
that generates a cylindrical panorama, a panning down-look camera yields the so-called
\emph{conical view}. In a conical view, the panorama coverage can be controlled by positioning the
scanning camera in an adjustable slanted down-look gazing angle. In this viewing arrangement,
stitching the single scan lines by panning the camera about the vertical axis through its
projection center yields a conical panorama with a single projection center \cite{neg01}. While
scanning systems involve complex designs, requiring high-precision electro-mechanical components,
panoramas can be generated by rearranging multiple cameras in various configurations that cover as
large as the entire 360 degrees viewing angle \cite{swa99}. Such multi-camera systems typically
have multiple projection centers, which induces parallax in overlapping areas of the images. Thus
the panoramic view can not be reconstructed by simply rearranging the pixels of the images. By
designing the imaging system with mirrors in precise positions, panoramas can be formed from the
images of multiple cameras while simulating the single projection center of a pinhole camera
\cite{nal98}. This design needs to be calibrated accurately, and unfortunately the cameras are
blocking a big portion of the entire viewable scene coverage. However, construction of a panoramic
view in a multi-cam system with single optical center can be readily done by arranging the pixels
according to the angles of the corresponding projection rays, independent of the range of the
corresponding scene point. Clearly, the depth information is lost through the scene to image
projection. Visual motion cues, e.g., optical flow, can be exploited for 3-D reconstruction of a
scene from images acquired at multiple views in our benthic habitat mapping application. In the
context of our 3-D mapping application, a conical panorama covering a large area of the terrain can
be readily constructed in a multi-cam system designed with a single projection center. In this
case, visual motion cues in the panoramas at two or more nearby viewing positions provide the
primary source of information for 3-D reconstruction. As in traditional flow-based,
correlation-based, or feature-based approaches, one deals with solving a two-dimensional
correspondence problem. In a system with multiple projection centers, say one per camera, the
construction of a panorama is more complex. Each scene point can potentially project onto the
panoramic view along multiple projection rays in accordance with stereoscopic imaging.
Consequently, one must resolve the depth of each scene point from the induced parallax-- e.g., by
establishing the correspondence between matching features in two or more views and triangulating
the projection rays-- before constructing the panoramic view. In the context of conical panoramic
imaging, we are now dealing with a stereo conical view in a system with multi projection centers.
In a stereo conical imaging system-- i.e., a multi-cam configuration with multiple projection
centers-- there is the advantage that each viewing position provides disparity cues encoding 3-D
depth information. Furthermore, exterior/extrinsic calibration to establish the position and pose
of each camera in the reference coordinate frame of the stereo conical imaging system enables
exploiting the epipolar geometry constraint, and thus reducing the correspondence problem to a 1D
search. Finally, computation of the motion from one to any nearby view can be done based on the
knowledge of 3-D depth (for a minimum of 3 scene points) at any of the two views-- e.g., the
closed-form solution to the exterior orientation \cite{hor87-1,hor87-2,mic00} --which can be done
more efficiently \cite{zha04} and often more robustly, in comparison to the standard optical
flow-based methods, or the well-known 8-point feature-based algorithm.

This study addresses the deployment of conical panoramic imaging systems for 3-D benthic habitat
mapping from flyover imagery. We start with describing a six-camera system originally designed for
the construction of panoramic views, treating the small distances among multiple projection centers
as negligible for scenes at a distance of no less than approximately $6\;[m]$ \cite{neg01}.
However, acquisition of images with high target details in scattering and absorbing medium, such as
in underwater, may often require viewing distances below the minimum range. We discuss the
shortcomings in the deployment of a multi projection center system for these cases. For robust
motion estimation, the advantages in exploiting the optical flow over a spherical image plane have
been demonstrated \cite{nel87}. In the same spirit, our theoretical results on the condition number
of the system matrix for the computation of motion parameters establishes that a conical panorama
provides superior performance with respect to images acquired over smaller fields of view
\cite{fir02}. The theoretical results are verified in an experiment with a real sequence.

The balance of this investigation deals with a 12-camera stereo conical imaging system,
specifically designed with distant projection centers, to enable 3-D reconstruction over a large
portion of the entire field of view (where there is an image overlap in a minimum of two nearby
cameras) at each position. A mathematical framework to study our system requires generalization of
the pinhole camera model. In contrast to the abstract generalized camera model by \cite{gro01}, we
propose a generalization that involves a more explicit representation, and thus a much simpler
modeling framework for panoramic imaging with multiple projection centers. Here, the projection
center and image plane are two arbitrary surfaces described by appropriate mapping functions.

While many panoramic imaging systems have been proposed, little attention has been given to
investigating the quantitative performance of these systems for various visual tasks, namely the
accuracy issue in 3-D reconstruction, motion estimation and self positioning that are of special
interest to our application \cite{fer00,har94,ple03}. A major contribution of this study is the
development of a theoretical framework that enables the assessment on any \emph{N-Ocular} vision
system. It suffices to represent the imaging system in terms of the projection matrices of its
component cameras. Consequently, we can give uncertainty ranges based on mathematical expression
for the covariance matrices of various estimation parameters: for 3-D position of a surface point,
motion parameters, and the position of the imaging system platform at each view. These theoretical
results are put to
test in various experiments with synthetic data, as well as real images and sequences.\\

%%%%?????????????????????????????????????????????????
%3-D Super-resolution mosaicing should come here!!!
%%%%?????????????????????????????????????????????????


%This thesis addresses the modeling, analysis, and performance evaluation of a multi-camera stereo conical imaging system for 3-D reconstruction from flyover imagery.

\section{Overview and Motivation}

Automated mapping of benthic habitats by the deployment of autonomous mobile platforms is critical
for sub-sea environmental monitoring, sea floor surveys for science studies (e.g. marine geology,
archeology), and inspection of off-shore structures, among many applications. A recent study has
explored the deployment of a stereovision system for 3-D high-resolution terrain mapping, namely
local areas in the order of roughly $100\times 100\;[ft^2]$-- e.g., shipwreck sites for
archeological studies and documentation, coral reefs for health assessment, and fisheries studies
\cite{neg03}. It has been determined that flying at an altitude of about $12\;[ft]$, a submersible
platform would need to record about 5000 frames at a rate of a pair per one foot distance traveled
over 50 swaths, providing sufficient overlap in the data that would give complete coverage.
Reducing the altitude by half to increase the resolution, four times as many images are to be
acquired. Maintaining the planned trajectory during image acquisition in the face of constant
current disturbances, either manually or automatically, is a rather complex task. At a average
daily cost of $\$15-\$50K$ for the support ship and submersible deployment, the economic factors
are also a serious consideration. Panoramic views have a significant advantage: They provide large
scene coverage at each position of the imaging system, which 1) reduces the number of viewing
positions for a fixed coverage area, 2) removes the burden of precise positioning to ensure a
minimum overlap in nearby frame, leading to a less complicated navigation problem, and 3) cuts down
the data acquisition time that translates to significant savings in operational cost. For a similar
operation described above as an example, less than 900 conical views acquired with a system similar
to the one investigated in this study can provide similar coverage with more accurate results.
Furthermore, the use of a conical imaging system can improve the trajectory execution and reduce
operation time considerably.

%The problem addressed in this thesis is primarily involved with the recovering the absolute motion
%of a flyover platform and the structure of an environment, using a multi-camera panoramic vision
%system. As a scenario consider a flyover operation of an airborne system over $100\times
%100\;[ft^2]$ area at the altitude of $11\;[ft]$ in the form of lawn-mower pattern to reconstruct
%the 3-D scene model. Roughly, 5100 stereo pairs is needed for a standard stereo-rig system
%\cite{neg03}. In contrast,

Important design features of the proposed multi-camera system are:

\bi
    \item \textit{Simplicity:} Integration of off-the-shelf CCD camera boards that are arranged in a desired alignment;
    \item \textit{Flexibility:} Resolution can be controlled by the number of cameras;
    \item \textit{Configurability:} Spatial arrangement of cameras can be varied to control the coverage, based on the operational range and task requirements;
    \item \textit{Cost:} By exploiting the exponential growth in computing power, speed and performance, increased imaging resolution is achieved by post-processing of images from very low-cost CCD cameras;
    \item \textit{Robustness:} Misalignment of cameras from design specifications is insignificant, and is corrected based on a one-time calibration process, which is also a requirement for alternative panoramic imaging
    technologies.
\ei

%In addition, the proposed system can be used as an aid for the automated navigation and mapping in
%the deployment of autonomous mobile platforms for sub-sea environmental monitoring, sea floor
%surveys for science studies (e.g. marine geology, archeology), and inspection of offshore
%structures. These require data acquisition at precise locations, usually resorting to the use of
%unmanned underwater vehicles either in the form of ROVs, or untethered AUVs. Vision sensing can
%provide high precision positioning and therefore accurate navigation and mapping over relatively
%smaller section of the sea floor for such applications.


The methods investigated in this thesis can be used to estimate the motion of a flyover mobile
robot (e.g., autonomous helicopter, and autonomous underwater vehicle (AUV)). Motion parameters
estimated from vision can be used to augment other motion sensors (e.g., inertial navigation system
(INS), and gyros) and also 3-D reconstructed scene can be utilized in 3-D exploration of an
environment, 3-D modeling, scene rendering, and various scientific studies that deal with
measurements from 3-D models \cite{kha99, neg98p}. Using recovered camera motions, and combining
the multiple 3-D views, a 3-D mosaic is constructed. This provides a compact, yet complete,
representation of all the recovered information from many images.

% It is noticed that most earlier works emphasize the land applications, and very few address the potential deployment in underwater or other medium with absorption and scattering properties.

\section{Related Works}

\subsection{Panoramic Imaging}
%%%%%%%%%%%%%%%%%%%% PANORAMIC
The history of \emph{panorama} starts with the work of \emph{Robert Barker} in 1767. But the first
panoramic camera was actually invented by \emph{Puchberger} of Austria in 1843. In
\emph{Puchberger}'s camera design, the lens pivots around an axis of rotation while the rest
remains stationary. Because of this, the maximum field of view of his camera is limited to
$150^\circ$ \cite{ben01-2}. The rotating camera invention of \emph{Garella} of England in 1857
extended the field of view of capture to full $360^\circ$. Unlike \emph{swing lens} and
\emph{rotating} panoramic cameras, which rely on moving parts, \emph{Sutton}'s panoramic camera,
invented in 1858, uses a very wide angle lens; A spherical lens filled with water to achieve the
field of view of $120^\circ$. \emph{Rees} is the first to patent an omnidirectional system using a
\emph{hyperboloid} mirror and a normal perspective camera in 1970 \cite{ree70}. This configuration,
which has a unique center of projection, has also been used in more recent systems (e.g.
\cite{svo98,yam93}). Another system which yields a unique center of projection is the
\emph{paraboloid} mirror and telecentric lens combination. In an interesting study by \emph{Baker}
and \emph{Nayar} \cite{bak99}, the complete class of single-lens/single-mirror catadioptric sensors
with a single viewpoint has been investigated. The reason a single viewpoint is so desirable is
that it is a requirement for the generation of pure perspective views from the sensed images. A
conic mirror setup has also been used \cite{yag94}, but this unfortunately does not produce images
with a single unique virtual projection center. As a result, images taken with such a conic mirror
setup contain parallax. There are also multiple camera systems with the advantage of producing
panoramic images at a higher resolution than their single-camera counterparts. For example,
\emph{McCutchen} described a dodecahedral imaging system in his 1997 patent \cite{mcc97}. In
principle, the twelve cameras are located in the centers of a pentagonal surfaces and positioned to
look outwards, thus covering the entire visual sphere. In 1996, \emph{Nalwa} patented a system of
cameras and mirrors that result in a single virtual projection center. The system was built with
four cameras looking up at an inverted pyramid whose four sides are mirrors. Of course, multiple
rotating camera configurations also exist \cite{ben98,ben96,mur95}. The design of todays panoramic
sensors can be categorized into mirror-based (e.g \cite{pri01,swa01,tan02,yok98}), wide-angle lens
(e.g. \cite{gro01,zhu00,zhu99}), and poly-camera (e.g. \cite{bak01,fer00,gro01,mcc97,nal98,ple03})
systems.

\subsection{Image Modeling}
%%%%%%%%%%%%%%%%% MODELLING BLOCK
%Understanding of the methods for recovering information from images requires a good understanding of image formation.
The relationship between the image and the scene has been the central subject of study in the
computer vision community for many years. While standard camera models such as pin-hole and affine
projection were explored in many studies (e.g. \cite{har00}), some researchers have generalized
these simpler models to analyze imaging systems for various designs and applications (e.g.
\cite{gro01,nay97-1,sch01,sei01,swa00}). The most general proposed camera model, in the form of a
mapping from light rays in the three-dimensional world onto a two-dimensional image, has been
studied in \cite{gro01,rad98}. For some applications, the use of a simpler and/or a more
constrained model can be more desirable (e.g. \cite{gup97,pel00}).


%Use of such camera model is not always the best choose, since the model too general for many
%applications. Instead a more constrained model would be preferred in some applications (e.g.
%\cite{gup97,pel00}).

%The equations of motion have been studied for a number of specific non-standard camera systems,
%including estimating optic flow from multi-camera systems in \cite{bak00-2,ple03}.


%There exist some works on motion estimation in such generalized camera models (e.g.
%\cite{bak00-2,ple03}.

%Solving for the structure of the scene and the motion of the imaging system is fundamentally a
%difficult problem \cite{adi89}. This limitation and many others have led to the development of new
%imaging systems. Recently new camera designs have appeared with different imaging geometries. For
%many purposes, these non-standard camera systems have significant advantages over pinhole cameras.
%The most general proposed camera model is in form of a general mapping from three-dimensional world
%onto a two-dimensional image plane. This has been studied in \cite{gro01,rad98}, but such general
%model can hardly be used practically.

%Development of algorithmic tools for interpreting images from a generalized camera model will lead
%to a better understanding of how to design camera systems for particular tasks.

%Panoramic imaging systems can overcome some of these problems, as an example the larger field of
%view of an omni-directional imaging system gives advantages for ego-motion estimation \cite{bro96}.
%\emph{Perspective projection model} is restrictive for such systems (e.g. \cite{fle95}). While
%standard camera models (e.g. pin-hole, affine projection) were explored in many literatures (e.g.
%\cite{}), some researchers generalized the simpler models to fit them to some specific applications
%and designs (e.g. \cite{gro01,nay97-1,sch01,sei01,swa00}). Generalization of the camera model can
%be done in radiometric and/or geometric aspects of it. Some models are to deal with the
%non-perfectness or modeling of complexity of todays cameras' parts such as compound or tick lenses
%(e.g. \cite{agg01}), while some others are to generalized the image formation by representing
%camera model as a mapping from three-dimensional world onto a two-dimensional image plane (e.g.
%\cite{gro01,sei01}). While many of proposed generalized models deal with a single center of
%projection, there exist a few models for the non-central projection cameras (e.g.
%\cite{gro01,ple03}).

\subsection{Sensitivity Analysis}
%%%%%%%%%%%%%%%%% ACCURACY ASSESSMENT BLOCK
Sensitivity analysis of the estimated parameters from a vision system is critical in many
applications, including visually guided precise positioning and/or mapping. In general, accuracy
assessment of a vision system is more difficult when the underlying vision algorithms are based on
nonlinear equations and/or there does not exist a closed-form solution. To deal with this, many
researchers have worked on evaluating the sensitivity of different algorithms under additive image
noise by simulating the process many times, and then estimating the covariance matrix of the
estimated parameters statistically (e.g. \cite{cro98,fir03-1,ple03}). Furthermore, only a few have
analyzed the accuracy that could be achieved with multi-camera configurations (e.g.
\cite{fer00,har94,ple03}). Some of this work is based on estimating the \emph{Fisher Information
Matrix} \cite{sor80} and then determining the lower bound for the covariance matrix of the
estimated parameters (e.g. \cite{ple03}).

%,fir04-1,fir04-2

\subsection{Super-Resolution Imaging}

Video images usually contain a large overlap between consecutive frames, and some region in the
scene often appears in several frames. Mosaicing is a way to combine information from multiple
frames in a video to make images with a larger FOV, giving us a more complete view of the scene.
The mosaicing process can use combinations of overlapping images, therefore improving the SNR of
the final mosaic \cite{ela97,zom98,zom00}.
% 1- You should end the previous paragraph with the fact that resolution in the finial mosaic is not uniform.
% 2- Talk about mosaicing in prev. para.
%%%%%%%%%%%%%%%%% SUPER RESOLUTION BLOCK
Super-resolution techniques can be utilized to overcome the non-uniform resolution of a
reconstructed mosaic (e.g. \cite{zom00}). The output is a mosaic with a uniform spatial resolution
and improved SNR by fusing the information from multiple images captured in different resolutions
from a scene \cite{ela97,zom98,zom00}. The majority of algorithms are based on the fundamental
constraints that the super-resolution image should generate the low-resolution input images when
appropriately warped and down-sampled to model the image formation process (e.g.
\cite{ela99,har97,ira93,pat97,sch96,sme00,zom00}). The non-uniform resolution video frames are the
result of projection of a high-resolution scene mosaic onto the image surface, followed by
sampling. The goal is to find the mosaic which fits this model. The major difference between most
of existing algorithms is in the optimization technique used for solving this set of equations,
and/or the constraints applied to the super-resolution image. Some of the algorithms are presented
in batch mode manner (e.g. \cite{del98}), while some other are implemented to be solved iteratively
(e.g. \cite{zom00}).

\subsection{Vision-Based Navigation}

%%%%%%%%%%%%%%%%% NAVIGATION/MAPPING BLOCK
Vision-based self-positioning and navigation involves the estimation of 3-D motion from
time-varying imagery (e.g. \cite{alo84,alo86,bru83,neg00p,ste96p,ste96}). Most techniques for this
purpose require knowledge of relevant 2-D geometric information in an image sequence, either the
correspondence of prominent features (e.g. \cite{blo88}) or a dense image motion, the so-called
\emph{optical flow} (e.g. \cite{bru83}). Panoramic imaging systems are used to overcome some of the
limitations of the ordinary vision systems for mapping and navigation (e.g.
\cite{arg01,cha00,neg01,nel87,win00,zhu00}). For example, it has been also shown that the
multi-camera configuration or panoramic imaging can provide a higher motion estimation and
therefore positioning accuracy (e.g. \cite{fir03-1,nel87,ped00}). Most previous vision systems for
navigation and mapping have been analyzed in a forward-look setup (e.g.
\cite{arg01,ben00,cha00,zhe92}). The nature of applications of interest to us demands the use of
down-look cameras \cite{fir03-1,neg01}.

%fir04-1,

%The difficulty, however, is in the computation of a sufficient accurate 2-D geometric information
%from radiometric data, the image volume $I(x,y,t)$, sampled over a spatiotemporal grid. This
%becomes a more serious problem for underwater sequences due to radiometric variations in images
%induced by nonuniform illumination from artificial sources installed on the vehicle, light source
%motion, medium attenuation and scattering, etc.

%Environmental information collection would involve exploiting all kinds of visual cues such as
%motion (e.g. \cite{}), stereo (e.g. \cite{}), contour (e.g. \cite{}), focus (e.g. \cite{}), and
%shading (e.g. \cite{}).

\section{Our Approach and Major Contributions}

The main goal of this dissertation, motivated by the need in a number of scientific studies, is to
design and analyze the performance of a robust vision system for short-distance automatic
navigation and 3-D terrain mapping from flyover imagery, acquired by the deployment of an
underwater submersible vehicle. To do this, we explore the robust estimation of all six motion
parameters as well as the reconstruction of 3-D scene structure. We investigate the use of a
combination of conventional cameras to generate high-resolution panoramas for mapping and
navigation applications. To analyze the multi-cam conical imaging system that has been designed and
built, a generalization of the pin-hole model is developed where the image plane and the projection
center are two general surfaces. Basic image properties such as scan lines and sensor element size
are redefined in this new framework. While the application of the model is demonstrated for several
sample cameras/image surfaces, conical imaging is analyzed in details. Our design takes advantage
of several scene observations at each time instant, to permit 3-D scene reconstruction from
multiple views. Furthermore, knowledge of the 3-D structure and the use of panoramic views enable a
much more accurate estimation of camera platform motion for positioning and 3-D mapping. In
particular a super-resolution algorithm is used to create 3-D mosaics with a uniform spatial
resolution. The major contributions of this work can be summarized as follows:

\bi

\item A new image modeling framework for the generalization of the pin-hole camera model (Chapter~\ref{chap:gen_cam_model})

\item Application of the framework to analyze a \emph{conical view} (Chapter~\ref{chap:conical_camera})

\item Theoretical foundation to analyze the accuracy in motion estimation, positioning, and 3-D reconstruction for a \emph{N-Ocular} vision system
(Chapter~\ref{chap:accuracy_analysis})

\item 3-D super-resolution mosaicing for a \emph{N-Ocular} vision system (Chapter~\ref{chap:3-D_mosaicing})

\item Various experimental results in support of theoretical developments to analyze the performance of the stereo conical
imaging system (Chapter~\ref{chap:experimental_results})

\ei

%The following approach is proposed:
%
%\bi
%    \item Use of poly-cameras to achieve to a higher accuracy in motion estimation and 3-D
%    reconstruction.
%    \item Use \emph{stereo conical view} to capture the scene video over a large \emph{FOV}.
%    \item Reconstruct 3-D scene structure using the \emph{stereo conical view} frames.
%    \item Exploit \emph{super resolution} technique to fuse \emph{conical views} to construct a high resolution 3-D mosaic.
%\ei

\section{Outline of Dissertation}

%In chapter 2, we provide a brief mathematical review of some fundamental concepts and methods, as
%well as define the notations used in the rest of the thesis.

%perspective projective camera and stereo camera models, in addition to common concepts and
%notations
%
%Furthermore, we present some topics that are not directly used in the approach proposed in this
%thesis, however, they demonstrate the contrast among different techniques.

In chapter 2, we introduce a new paradigm in panoramic imaging-- the so-called \emph{conical
view}-- suitable for motion estimation from flyover imagery systems. We will show that the
application of the system is limited to either far range, or knowledge of a dense depth map of the
scene. These limitations are addressed in chapter 4 by introducing the \emph{stereo conical imaging
system}.

In chapter 3, a generalization of the pin-hole model is given where the image plane and the
projection center are two general surfaces. Basic image properties such as scan lines and sensor
element size are redefined in this new framework, and the application of the model is demonstrated
for several sample cameras/image surfaces.

In chapter 4, we introduce the \emph{stereo conical imaging system} for 3-D reconstruction and
motion estimation, represented in the generalized camera framework.

In chapter 5, the sensitivity analysis of a \emph{N-Ocular} vision system is presented. While the
application is demonstrated for several sample camera configurations, the conical imaging system is
analyzed in details.

In chapter 6, we present an enhanced 3-D super-resolution algorithm to handle images with large
variation in illumination. This is rather important for many applications where scene lighting may
change with camera position, e.g. in underwater where rapid attenuation of light often induces
significant variations in brightness of images acquired from multiple camera poses.

Chapter 7 is dedicated to the results from a number of experiments on 3-D motion estimation and
scene reconstruction with real image sequences. The goal is to show how the stereo conical imaging
system performs on real data by presenting both quantitative and qualitative results.

%the implementation of the framework discussed in the previous chapters and to the presentation of
%the experiments. We also show the results for real images sequences taken in the lab.

In chapter 8, we summarize our contributions. Furthermore, we give some thoughts on possible future
extensions.  Finally, the appendices include description of some of the algorithms we have
utilized. These topics can be found in the literature, but are given for completeness.

We feel obliged to warn the user of several occurrences of notational abuse, particularly in
chapter 5. In some cases, several variables have been combined to define a new one-- e.g., a few
vectors arranged to define a larger vector-- with the same notation utilized for some other
variable elsewhere. Part of this problem is a direct result of trying to adhere to a certain
convention in defining our variables. At other times, e.g., when defining the eigenvalue/singular
value decomposition of two matrices, we have used the same variables in the decomposition for both
matrices, without making a distinction with sub/superscripts. This is primarily to avoid later
confusions when defining other variables, with one or more sub/superscripts, e.g., the
row/column/elements of these matrices. Overall, we have felt that such abuse of notation would be
less confusion than adapting other conventions. In fact, in most case, such repeated definitions
are local, and potential ambiguities should be overcome with some little effort.
