\chapter{Xylo-Bot: An Interactive Music Teaching System} 
A novel interactive human-robot music teaching system design is presented in 
this chapter. In order to make the robot play the xylophone properly, several things need 
to be done. First is to find a proper xylophone with correct timber; 
second, we have to arrange the xylophone in the proper position in front of the robot 
to make it visible and be reachable to play; finally, design the 
intelligent music system for NAO.\\

\section{NAO: A Humanoid Robot}
We used a humanoid  robot called NAO developed by Aldebaran Robotics in France. 
NAO is 58 cm (23 inches) tall, with 25 degrees of freedom. This robot 
can conduct most human behaviors. It also features an onboard multimedia 
system including four microphones for voice recognition and sound localization, 
two speakers for text-to-speech synthesis, and two HD cameras with maximum image 
resolution 1280 x 960 for online observation. As shown in Figure \ref{nao_body}, these 
utilities are located in the middle of the forehead and the mouth area. NAOâ€™s 
computer vision module includes facial and shape recognition units. By using the 
vision feature of the robot, the robot can see the instrument 
from its lower camera and be able to implement an eye-arm self-calibration 
system which allows the robot to have real-time micro-adjustment of its 
arm-joints in case of off positioning during music playing.\\
\\

\begin{figure}[tbp]
	\begin{center}
		\begin{tabular}{c}
			\epsfig{figure=./chapters/fig/naobody.eps, scale = .8}\label{nao_body} \\
		\end{tabular}
		\caption{A Humanoid Robot NAO: 25 Degrees of Freedom, 2 HD Cameras and 4 Microphones} \label{nao_body}
	\end{center}
\end{figure}

The robot arms have a length of approximately 31 cm. Each arm has five degrees 
of freedom and is equipped with sensors to measure the position of each 
joint. To determine the pose of the instrument and the mallets' heads, the robot 
analyzes images from the lower monocular camera located in its head, which has a 
diagonal field of view of 73 degrees. These dimensions allow us to choose a 
proper instrument presented in the next section.\\
% show nao's open arms pic here, use the pic from german group
%\begin{figure}[tbp]
%	\begin{center}
%		\begin{tabular}{c}
%			\epsfig{figure=./chapters/fig/naobody.eps, scale = 0.45}\label{nao_open_arm} \\
%		\end{tabular}
%		\caption{NAO Open Arm with Playable Positions} \label{nao_open_arm}
%	\end{center}
%\end{figure}

The four microphone locations embedded on the toy or NAO's head can be seen in figure \ref{mic_loc}. 
According to the official Aldebaran documentation, these microphones have sensitivity 
of 20mV/Pa +/-3dB at 1kHz, and an input frequency range of 150Hz - 12kHz. Data 
will be recorded as a 16 bit, 48000Hz, 4 channels wav file which meets the 
requirements for designing the online feedback audio score system described below.\\
% show nao's head with mic details here
\begin{figure}[tbp]
	\begin{center}
		\begin{tabular}{c}
			\epsfig{figure=./chapters/fig/mic.eps, scale = 1.2}\label{mic_loc} \\
		\end{tabular}
		\caption{Microphone locations on NAO's head} \label{mic_loc}
	\end{center}
\end{figure}

\section{Accessories}
The purpose of this study is to have a toy-size humanoid robot play music. Some necessary accessories 
needed to be purchased and made before the robot was able to play music. 
All accessories will be discussed in the following sections.\\

\subsection{Xylophone: A Toy for Music Beginner}
In this system, due to NAO's open arms' length, we choose a Sonor Toy Sound SM 
soprano-xylophone with 11 sound bars of 2 cm in width. The instrument has a size of 
31 cm x 9.5 cm x 4 cm, including the resonating body. The smallest sound bar is 
playable in an area of 2.8 cm x 2 cm, the largest in an area of 4.8 cm x 2 cm. The 
instrument is diatonically tuned in C-Major/a-minor. For the beaters/mallets, we used 
the pair that came with the xylophone with a modified 3D printed grip (details in next 
subsection) to allow the robot's hands to hold them properly. The mallets 
are approximately 21 cm in length and include a head of 0.8 cm radius. See Figure \ref{xylo640}.\\ 

\begin{figure}[tbp]
	\begin{center}
		\begin{tabular}{c}
			\epsfig{figure=./chapters/fig/xylo640.eps, scale = .6}\label{xylo640} \\
		\end{tabular}
		\caption{Actual Xylophone and Mallets from NAO's Bottom Camera} \label{xylo640}
	\end{center}
\end{figure}

The 11 bars of the xylophone represent 11 different notes (11 frequencies) which covers 
approximately a one and a half octave scale starting from C6 to F7. \\
%(see figures or a table with different frequencies somewhere)

\subsection{Mallet Gripper Design}
According to the size of Nao's hands, we designed and 3D printed a pair of gripers to 
have the robot be able to hold the mallets properly. All dimensions can be found 
in Figure \ref{griper}.\\

\begin{figure}[tbp]
	\begin{center}
		\begin{tabular}{c}
			\epsfig{figure=./chapters/fig/grip.eps, scale = 0.8}\label{griper} \\
		\end{tabular}
		\caption{Mallet Griper} \label{griper}
	\end{center}
\end{figure}

\subsection{Instrument Stand Design}
A wooden base was designed and laser cut to hold the instrument in the proper place 
for the robot to be able to play music. All dimensions can be found in 
Figure \ref{stand}.\\

\begin{figure}[tbp]
	\begin{center}
		\begin{tabular}{c}
			\epsfig{figure=./chapters/fig/left_view.eps, scale = 0.3}\label{left} \\
			(a)\\
			\epsfig{figure=./chapters/fig/top_view.eps, scale = 0.3
			} \label{top}\\
			(b)\\
			\epsfig{figure=./chapters/fig/front_view.eps, scale = 0.6} \label{front}\\
			(c)
		\end{tabular}
		\caption{Instrument Stand: (a) Left View (b) Top View (c) Front View.} \label{stand}
	\end{center}
\end{figure}


\section{Module-Based Acoustic Music Interactive System Design}
In this section, a novel module-based robot-music teaching system will be presented. 
Three modules have been built in this intelligent system including module 1: eye-hand 
self-calibration micro-adjustment; module 2: joint trajectory generator; and 
module 3: real time performance scoring feedback. See Figure \ref{module}.\\

\begin{figure}[tbp]
	\begin{center}
		\begin{tabular}{c}
			\epsfig{figure=./chapters/fig/module_blocks.eps, scale = .5}\label{module} \\
		\end{tabular}
		\caption{Block Diagram of Module-Based Acustic Music Interactive System} \label{module}
	\end{center}
\end{figure}

\subsection{Module 1: Eye-hand Self-Calibration Micro-Adjustment}
Knowledge about the parameters of the robot's kinematic model is essential for 
tasks requiring high precision, such as playing the xylophone. While the kinematic 
structure is known from the construction plan, errors can occur, e.g., due to the 
imperfect manufacturing. After multiple rounds of testing, it was found the targeted angle chain 
of arms never actually equals the returned chain. We therefore used a 
calibration method to accurately eliminate these errors.\\
 

\subsubsection{A. Color-Based Object Tracking}
To play the xylophone, the robot has to be able to adjust its motions according to
the estimated relative position of the instrument and the heads of the beaters it is 
holding. To estimate these poses, adopted in this thesis, we 
used a color-based technique.\\
The main idea is, based on the RGB color of the center blue bar, given a hypothesis 
about the instrument's pose, one can project the contour of the object's model into the 
camera image and compare them to actually observed contour. In this way, it is possible 
to estimate the likelihood of the pose hypothesis. By using this method, it allows
the robot to track the instrument with very low cost in real-time. See Figure \ref{color_detection}\\
\begin{figure}[tbp]
	\begin{center}
		\begin{tabular}{c}
			\epsfig{figure=./chapters/fig/blue.eps, scale = 0.3}\label{single_color_a} \\
			(a)\\
			\epsfig{figure=./chapters/fig/all_color.eps, scale = 0.3
			} \label{all_color_b}\\
			(b)\\
			\epsfig{figure=./chapters/fig/color_detection.eps, scale = 0.6} \label{color_detection_c}\\
			(c)
			\end{tabular}
			\caption{Color Detection From NAO's Bottom Camera: (a) Single Blue Color Detection (b) Full Instrument Color Detection (c) Color Based Edge Detection.} \label{color_detection}
	\end{center}
\end{figure}

\subsubsection{B. Calibration of Kinematic Parameters}
(In progress, will not present in this version. The idea is to use both positions 
of the instrument and beaters' heads to compute a suitable 
beating configuration for arm kinematics chain for each sound bar. Suitable means that the beater's 
head can be placed on the surface of the sound bar at the desired angle. From 
this configuration, the control points of a predefined beating motion are updated.)\\ 

\subsection{Module 2: Joint Trajectory Generator}
Our system parses a list of hex-decimal numbers (from 1 to b) to obtain the sequence
of notes to play. It converts the notes into a joint trajectory using the beating
configurations obtained from inverse kinematics as control points. The timestamps
for the control points will be defined by the user in order to meet the experiment requirement.
The trajectory is then computed using Bezier interpolation in joint space by the
manufacturer-provided API and then sent to the robot controller for execution. In this
way, the robot plays in-time with the song.\\

\subsection{Module 3: Real-Time Performance Scoring Feedback}
The purpose of this system is to provide a real-life interaction experience using 
music therapy to teach kids social skills and music knowledge.  In this scoring 
system, two core features were designed to complete the task: 1) music detection;
2) intelligent scoring-feedback system.\\


\subsubsection{A. Music Detection}
Music, in the understanding of science and technology, can be considered as a combination 
of time and frequency. In order to make the robot detect a sequence of frequencies, we adopted the 
short-time Fourier transform (STFT) to this audio feedback system. This allows the robot to 
be able to understand the music played by users and provide the proper feedback as
a music teaching instructor.\\

The short-time Fourier transform (STFT) , is a Fourier-related transform used to 
determine the sinusoidal frequency and phase content of local sections of a signal 
as it changes over time. In practice, the procedure for computing STFTs is to divide 
a longer time signal into shorter segments of equal length and then compute the 
Fourier transform separately on each shorter segment. This reveals the Fourier 
spectrum on each shorter segment. One then usually plots the changing spectra as 
a function of time. In the discrete time case, the data to be transformed could 
be broken up into chunks or frames (which usually overlap each other, to reduce 
artifacts at the boundary). Each chunk is Fourier transformed, and the complex 
result is added to a matrix, which records magnitude and phase for each point in 
time and frequency. This can be expressed as:
\\

${\displaystyle \mathbf {STFT} \{x[n]\}(m,\omega )\equiv X(m,\omega )=\sum _{n=-\infty }^{\infty }x[n]w[n-m]e^{-j\omega n}}$
\\

likewise, with signal x[n] and window w[n]. In this case, m is discrete and $\omega$ 
is continuous, but in most typical applications, the STFT is performed on a computer 
using the Fast Fourier Transform, so both variables are discrete and quantized.\\
The magnitude squared of the STFT yields the spectrogram representation of the Power 
Spectral Density of the function:
\\

${\displaystyle \operatorname {spectrogram} \{x(t)\}(\tau ,\omega )\equiv |X(\tau ,\omega )|^{2}}$\\

After the robot detects the notes from user input, a list of hex-decimal number will be
returned. This list will be used in two purposes: 1) to compare with the target list
for scoring and sending feedback to user; 2) used as a new input to have
robot playback in the game session as discussed in the next chapter.\\

\begin{figure}[tbp]
	\begin{center}
		\begin{tabular}{c}
			\epsfig{figure=./chapters/fig/stft.eps, scale = 1.5}\label{stft} \\
		\end{tabular}
		\caption{Melody Detection with Short Time Fourier Transform} \label{stft}
	\end{center}
\end{figure}

\subsubsection{B. Intelligent Scoring-Feedback System}
In order to compare the detected notes and the target notes, we used an algorithm
which is normally used in information theory linguistics called Levenshtein Distance.
This algorithm is a string metric for measuring the difference between two sequences.\\

In our case, the Levenshtein distance between two string-like hex-decimal numbers 
${\displaystyle a,b}$ (of length ${\displaystyle |a|}$ and ${\displaystyle |b|}$ respectively) 
is given by ${\displaystyle \operatorname {lev} _{a,b}(|a|,|b|)}$ where
\\

${\displaystyle \qquad \operatorname {lev} _{a,b}(i,j)={\begin{cases}\max(i,j)&{\text{ if }}\min(i,j)=0,\\\min {\begin{cases}\operatorname {lev} _{a,b}(i-1,j)+1\\\operatorname {lev} _{a,b}(i,j-1)+1\\\operatorname {lev} _{a,b}(i-1,j-1)+1_{(a_{i}\neq b_{j})}\end{cases}}&{\text{ otherwise.}}\end{cases}}}$\\
\\

where ${\displaystyle 1_{(a_{i}\neq b_{j})}}$ is the indicator function equal to 0 when 
${\displaystyle a_{i}=b_{j}}$ and equal to 1 otherwise, and ${\displaystyle \operatorname {lev} _{a,b}(i,j)}$ 
is the distance between the first ${\displaystyle i}$ characters of ${\displaystyle a}$ and the
first ${\displaystyle j}$ characters of ${\displaystyle b}$.

Note that the first element in the minimum corresponds to deletion (from ${\displaystyle a}$ to 
${\displaystyle b}$), the second to insertion and the third to match or mismatch, depending on 
whether the respective symbols are the same. Table \ref{LD} demonstrates how to apply this
principle in finding the Levenshtein distance of two words "Sunday" and "Saturday".\\

\begin{figure}[tbp]
	\begin{center}
		\begin{tabular}{c}
			\epsfig{figure=./chapters/fig/example_LD.eps, scale = .6}\label{LD} \\
		\end{tabular}
		\caption{An Example of Compute Levenshtein Distance for "Sunday" and "Saturday"} \label{LD}
	\end{center}
\end{figure}

Based on the real life situation, we defined a likelihood margin for determining whether the result
is good or bad: \\
  
${likelihood = \dfrac{len(target) - lev_{target,source}}{len(target)}}$\\

where if the likelihood is greater than 66\% ~ 72\%, the system will consider it as a good result.
This result will be passed to the accuracy calculation system to have the robot decide whether it
needs to add more dosage to the practice. More details will be discussed in the next chapters
as it relates to the experiment design.\\

\section{Summary}
In this chapter, based on the purpose of this research, we have discussed both hardware
and software design for the experiment sessions.\\

From Chapter One, we determined to have NAO as a music teaching instructor be able
to both teach children simple music and deliver social content simultaneously.
In order to have the system ready, we first chose the proper agent, a robot named NAO, which is
kid-friendly and has complex social abilities. Second, based on the size of the robot, some necessary
accessories were purchased and handcrafted. A toy-sized color coded xylophone became the
best option and based on the size and position, a wooden based xylophone stand was 
customized and assembled. Due to the limitation of NAO's hand size, a pair of mallet gripers 
were 3-D printed and customized. Last, an intelligent module-based acoustic music 
interactive system was designed fully from scratch. With this haradware and software, 
three modules were designed to have the robot play, listen and teach the music freely. This allows
NAO to become a great companion for children in both music learning and social life. \\

Module 1 provides an autonomous self awareness positioning system for the robot to localize
the instrument and make micro adjustment for arm joints in order to play the note bar properly.
Module 2 allows the robot to be able to play any customized song of the user's request. This
means that any songs which can be translated to either C-Major or a-minor key can have a well-trained
person type in the hex-decimal playable score and allow the robot to be able to play it in seconds. Module 3
is designed for providing real life music teaching experience for system users. Two key features
of this module are designed: music detection and smart scoring feedback. Short time Fourier transform
and Levenshtein distance are adopted to fulfill the requirement which allows the robot to understand
music and provide proper dosage of practice and oral feedback to users.\\


