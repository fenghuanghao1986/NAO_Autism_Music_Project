\chapter{Protocol 1: Acoustic Music Teaching Experiment Design Data Acquisition and Result}
A few questions will be answered in terms of social skills in this chapter:
1) Turn Taking: How well kids with autism behave during the teaching and learning
process as compared to the TD group, e.g., kid listens to the instruction or demonstration 
from the robot before he/she plays the instrument;
2) Joint Attention and Eye-Gaze Attention: How well kids with ASD follow instructions 
and adapt hints as compared to TD kids, e.g., kid follows hitting positions demonstrated 
by the robot or follows the change of eye colors;
3) Motor Control: How well ASD kids play the xylophone in terms of volume, pitch and
accuracy compared to the TD group, e.g., a good multiple strikes should be recognized by 
STFT as a sequence of frequencies (design in previous chapter);
4) Engagement and Event Based Emotion: How kids engage with different music teaching
events and what emotions are displayed during these events, e.g., using EDA signal to
find out the emotions regarding different situations such as having a hard time
memorizing a sequence of notes;
5) Facial Expression and Emotions Correlation: (will not be presented in this version);
6) Music Emotions and Feelings: How kids perceive emotions in small pieces of music 
within different melodies, e.g., music in different keys.\\

\section{Room Setup and Participants}
Figure \ref{room} shows the experiment 
\begin{figure}[tbp]
	\begin{center}
		\begin{tabular}{c}
			\epsfig{figure=./chapters/fig/room.eps, scale = .6}\label{room} \\
		\end{tabular}
		\caption{Schematic robot-based therapy session and video capturing setting} \label{room}
	\end{center}
\end{figure}
% describe and show the room orgnization and describer subjects' detail here

\section{Experiment Design}
In order to collect all the data for answering the questions above, a set of
intervention sessions were designed using music therapy concepts. 6 - 7 sessions
were created for the ASD group and 2 sessions for the TD group. For the ASD group, entire
sessions were divided into 3 parts: baseline session, intervention sessions
and exit session. Similar to the ASD group, the TD group only includes a baseline session
and an exit session for comparison purposes. In addition, a social music
game play was also included in each session for system testing and entertainment
purposes.\\

\subparagraph{Baseline Session: }Participants were asked to follow all the 
instructions contained in all the practices from the following intervention session 
including single bar strike, multiple bars strike, half song play and the whole
song play. We choose a very popular kid's song "Twinkle Twinkle Little Star" 
for this specific session due to how well-known this song is in almost 
everyone's childhood.\\

\subparagraph{Intervention Sessions: }These sessions are assigned to ASD group
particularly and include single strike with color hints, multiple strikes with
colors hint, half-song practice and whole song practice. In this part, a special
participant-selected song will be used through the rest of the sessions. In the second
half of this intervention session set, single/multiple strikes were also covered
before the half/full song practice in order to have participants use the
color matching technique during the high level music play due to a lack of professional
music background knowledge. In addition, starting from session 2, a single strike
warm up practice was added before the formal music practice starts. This particular
practice was designed to have better motor control for ASD group, so that
the robot is able to recognize notes properly and deliver the concept in telling the
difference between "making a sound" and "playing a musical note". \\

\subparagraph{Exit Session: }Both groups were assigned to go through the same steps
as the baseline session in choice of their own songs. We would like to see the 
difference between two groups in learning a beloved song by their own.\\

Due to the difficulty of user selected songs and different performance scores of
participants, the total session numbers can be various: 6 visits will be the minimum
requirement for ASD group with the total number of visits not going beyond 8 times. More detailed 
experiment design is shown in the table below:\\

% still working on this table design
% show all experiment setup table here
%\begin{table}
%	\begin{center}
%		\caption{Performance Comparison Between the Two Methods and the
%			Manually Labeled Features Based on the Average MSE (pixels) Over All
%			70 Subjects.}
%		\begin{tabular}{|c|c|c|c|}
%			\hline
%			Method&Ave.MSE &Ave. MSE&\% of cases\\
%			&for 49 subjects&for 21 subjects&which have\\
%			&out of 70&out of 70&minimum error\\
%			\hline
%			Standard&&&\\
%			ASM & 70.3 (pixels)& 23.3 (pixels)& 30.0\% \\
%			\hline
%			Enhanced&&&\\
%			ASM & 40.0 (pixels)& 31.9 (pixels)& 70.0\% \\
%			\hline
%		\end{tabular}
%		\\
%		\label{table_2}
%	\end{center}
%\end{table}

\section{Methodology and Experiment Result}

\subsection{Social Aspects Annotation and Coding Methods}
This part is to describe how to do video coding in these social features.
Basically just post process by watching the front face videos and by listen to the
audio to code how the social behaviors been performed. Have to copy some of the stuff
from previous studies.
\subparagraph{Turn-Taking Coding}
\subparagraph{Joint Attention Coding}

\subsection{Motor Control Scoring System}
This is related to hitting practice and play practice in real time during the session.
Need to have connections with the module 2 real time performance scoring feedback system.
Describe the how the flow goes during the session and how to have small add on dosage
if the accuracy not good. 
Also have to make up a story when it goes to the half/whole song, how to manage that
if the kid cannot play well, just cut the whole chunk into small continuous pieces
and use the same methods and scoring system to deliver the idea.

\subsection{Emotion Classification}
Since we developed our emotion classification method based on the time-frequency analysis of the
EDA signals, the main properties of the continuous wavelet transform assuming complex Morlet
wavelet is first presented here. Then, the pre-processing steps, as well as the wavelet-based
feature extraction scheme, are discussed. Finally, we briefly review the characteristics of the
support vector machine as the classifier used with our approach.\\

\subparagraph{A. Continuous Wavelet Transform}
The EDA data recorded using the SC sensors are categorized as non-stationery signals\cite{AmbulatorySys2003, EmotionalState2013}. Hence, multiresolution analysis 
techniques are essentially suitable to study the qualitative components of these 
kinds of bio-signals \cite{AmbulatorySys2003}.  Note that continuous wavelet transform 
(CWT) is one of the strongest and most widely used analytical tools for multiresolution 
analysis. CWT has received considerable attention in processing signals with 
non-stationary spectra \cite{WaveletFilter1992, SignalDecomp1989}; therefore, it is 
utilized here to perform the time-frequency analysis of the EDA signals. In contrast 
to many existing methods that utilize the wavelet coefficients of the raw signal to 
extract features, our proposed method is essentially based on the spectrogram of the 
original data in a specific range of frequency (0.5, 50)Hz, which provides more 
information for other post-processing steps (i.e., feature extraction and classification).
We apply the wavelet transform at various scales corresponding to the 
aforementioned frequency range to calculate the spectrogram of the raw signal 
(i.e., Short Time Fourier Transform (STFT), can also be used to calculate the 
spectrogram of the raw signal). In addition, as opposed to many related studies 
that utilize real-valued wavelet functions for feature extraction purposes, we have 
employed the complex Morlet (C-Morlet) function with the proposed approach, as it 
takes into account both the real and imaginary components of the raw signal, leading 
to a more detailed feature extraction.
The wavelet transform of a 1-D signal provides a decomposition of the time-domain 
sequence at different scales, which are inversely related to their frequency contents\cite{SignalDecomp1989, ContinuWavelet2009} . This requires the time-domain signal under 
investigation to be convolved with a time-domain function known as “mother wavelet”. 
The CWT applies the wavelet function at different scales with continuous time-shift 
of the mother wavelet over the input signal. As a consequence, it helps represent 
the EDA signals at different levels of resolution. For instance, it results in large 
coefficients in the transform domain when the wavelet function matches the input 
signal, providing a multiscale representation of the EDA signal.
Using a finite energy function $\Psi$(t) concentrated in the time domain, the CWT of 
a signal x(t) is given by X($\alpha$,b) as follows \cite{WaveletFilter1992}:

$X(a,b) = \int_{-\infty}^{+\infty}x(t)\frac{1}{\sqrt{a}} \Psi (\frac{t-b}{a}) dt$\newline

where, $\alpha$, is the scale factor and represents dilation or contraction of the wavelet 
function and b is the translation parameter that slides this function on the 
time-domain sequence under analysis. Therefore, $\Psi$($\alpha$,b) is the scaled and translated 
version of the corresponding mother wavelet. “*” is the conjugation operator.
Note that the wavelet coefficients obtained from Eq. (1) essentially evaluate the 
correlation between the signal x(t) and the wavelet function used at different 
translations and scales. This implies that the wavelet coefficients calculated 
over a range of scales and translations can be combined to reconstruct the original 
signal as follows:

$x(t) = \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} X(a,b) \Psi (\frac{t-b}{a}) dadb$\newline

\subparagraph{B. Wavelet-Based Feature Extraction}
The time-frequency analysis of various bio-signals has been addressed in numerous 
related literature \cite{MultKernel2016, FFTSync2017}(Zhang et al., 2009). 
It has been shown that the wavelet-domain feature space can improve the recognition 
performance of different human activities using the signals emanated from the body 
responses. Therefore, it essentially enhances the classification performance due to 
the more distinctive feature space provided. 
In this paper, we focus on the time-frequency analysis of the EDA signal to provide 
a new feature space based on which an emotion classification task can be done. As 
opposed to some related studies that employ the raw time-domain signals for 
classification purposes \cite{ArousalValence2017, EmotionClass2012}, we use the 
amplitude of the CWT of the EDA signals to generate the features and drive the 
classifier. Working in the wavelet-domain is essentially advantageous since the 
wavelet transform probes the given signal at different scales, extracting more 
information for other post-processing steps. In addition, the localized support 
of the wavelet functions enables CWT-based analysis to match to the local variations 
of the input time sequence \cite{WaveletFilter1992}. As a result, a more detailed 
representation of the signal is provided in comparison with the raw time-domain signal.

Figure \ref{cwt_eda} shows the amplitude of the CWT of a sample EDA signal at different scales 
using a complex Morlet (C-Morlet) wavelet function. As can be seen, due to the localization 
property of the CWT, different structures of the input signal are extracted at each level 
of decomposition, providing useful information for analyzing the recorded EDA signals.
In this work, we have employed the C-Morlet wavelet function to process the acquired EDA 
signals, as it has been well used for time-frequency analysis of different bio-signals 
and classification \cite{MultKernel2016}. Figure \ref{feature} shows the wavelet-based 
feature extraction. Note that the impact of different families of the wavelet functions 
(e.g., Symlets, Daubechies, Coiflets) on the emotion classification will be evaluated 
in the next subsection. The equation of the C-Morlet mother wavelet with fc as its central 
frequency and fb as the bandwidth parameter is given as follows:

$\Psi (t) = \frac{\exp(-t^2/f_b)}{\sqrt(\pi f_b)} \exp (j2\pi f_c t)$\newline

\begin{figure}[tbp]
	\begin{center}
		\begin{tabular}{c}
			\epsfig{figure=./chapters/fig/cwt_eda.eps, scale = .7}\label{cwt_eda} \\
		\end{tabular}
		\caption{The CWT of a typical EDA signal using the C-Morlet mother wavelet. Different scales of the wavelet functions are convolved with the original EDA signal to highlight different features of the raw data. As can be seen inside the bottom box, when the scaling parameter of the wavelet function increases, the larger features of the input signal are augmented. On the other hand, the detailed structures of the signal are better extracted when the scaling factor decreases. } \label{cwt_eda}
	\end{center}
\end{figure}

\begin{figure}[tbp]
	\begin{center}
		\begin{tabular}{c}
			\epsfig{figure=./chapters/fig/wavelet_feature.eps, scale = .8}\label{feature} \\
		\end{tabular}
		\caption{The wavelet-based feature extraction. Using the C-Morlet mother wavelet, the real and imaginary wavelet coefficients are calculated at different scales. Then, the amplitude of these coefficients is calculated to provide the corresponding spectrogram. This spectrogram is then used as the feature space.} \label{feature}
	\end{center}
\end{figure}

\subparagraph{C. Support Vector Machine}
The SVM classifier tends to separate data \\
${D = \{x_i, y_i\}^{N}_{i=1}, x_i\in^{d}, y_i\in\{-1,+1\}}$
by drawing an optimal hyperplane <w,x>+b=0 between classes such that the margin between 
them becomes maximum \cite{SupportVector1995}. With reference to Figure \ref{svm}, H1 and H2 are 
the supporting planes and the optimal hyperplane (OH) splits this margin such that it 
stands at the same distance from each supporting hyperplane. This implies that the 
margin between H1 and H2 is equal to 2 / $\parallel$w$\parallel$.
In terms of linearly separable classes, the classifier is obtained by maximizing the 
margin 2 / $\parallel$w$\parallel$, which is equivalent to minimizing $\parallel$w$\parallel$ / 2 
with a constraint in convex quadratic programming (QP) as follows:

$\min \frac{1}{2}||w||^2 s.t. y_i(<w, x_i> + b) \ge 1$\newline

\begin{figure}[tbp]
	\begin{center}
		\begin{tabular}{c}
			\epsfig{figure=./chapters/fig/svm.eps, scale = 1}\label{svm} \\
		\end{tabular}
		\caption{Canonical SVM for classifying two linearly separable classes. The decision boundary is shown by OH. Two hyperplanes H1 and H2 pass the support vectors that are circled inside the figure.} \label{svm}
	\end{center}
\end{figure}

where, w and b are the parameters of the hyperplane and <.,.> is the notation of 
the inner product.
However, different classes are seldom separable by a hyperplane since their samples 
are overlapped in the feature space. In such cases, a slack variable ${\xi_i\geq}$ 0 and a 
penalty parameter C $\geq$ 0 are used with the optimization step to obtain the best feasible 
decision boundary. It is given as:

$\min \frac{1}{2}||w||^2 + C(\Sigma_{i=1}^{N} \xi_i) s.t. y_i(<w, x_i> + b) \ge 1 - \xi_i$\newline

Usually, various kernel functions are used to deal with the nonlinearly separable data. 
As a result, the original data xi is mapped onto another feature space through a 
projection function ${\varphi(\cdot)}$. It is not necessary to exactly know the equation of the 
projection ${\varphi(\cdot)}$, but one can use a kernel function ${k(x_i,x_j)=<\varphi(x_i),\varphi(x_j)>}$. 
This function is symmetric and satisfies the Mercer’s conditions. The Mercer’s conditions determine 
if a candidate kernel is actually an inner-product kernel. Let ${k(x_i,x_j)}$ be a continuous 
symmetric kernel defined in the closed interval ${t_1\leq t\leq t_2}$, the kernel can be expanded 
into series ${\Sigma_(n=1)^\infty = \lambda_n\varphi_n(x_i)\varphi_n(x_j)}$, where 
${\lambda_n > 0}$ are called eigenvalues and functions $\varphi$n are called eigenvectors in the expansion. 
The fact that all the eigenvalues are nonnegative means that the kernel is positive 
semidefinite \cite{SupportVector1995}. 
To maximize the margin, H1 and H2 are pushed apart until they reach the support vectors 
on which the solution depends. To solve this optimization problem, the Lagrangian dual 
of Eq. (5) is used as follows:

$\max_\alpha \Sigma_{i=1}^{N} \alpha_i - \frac{1}{2} \Sigma_{i=1}^{N} \Sigma_{j=1}^{N} y_i y_j \alpha_i \alpha_j k(x_i, x_j)$\newline
$s.t. 0 \le \alpha_i \le C, \Sigma_{i=1}^{N} \alpha_i y_i = 0, i = 1, ..., N$\newline

where, ${\alpha_i}$s are the Lagrangian multipliers in which just a few number of them are 
non-zero. These non-zero values are corresponding to the support vectors determining 
the parameters of the hyperplane ${w = \Sigma_(i=1)^N\alpha_iy_ix_i }$. Therefore, the label 
of the test sample ${(y_z)}$ is given by:
$y_z = sgn(\Sigma_{i=1}^{N} \alpha_i y_i k(x_i, z)) + b$\newline

\subsection{Dialog System}

\subsubsection{Speech Recognition}
http://doc.aldebaran.com/2-1/naoqi/audio/alspeechrecognition.html

\subsubsection{Dynamic Oral Feedback}
reason to design the dynamic feedback. 

\