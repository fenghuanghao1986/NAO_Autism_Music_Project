\documentclass[conference]{IEEEtran}
\usepackage{epsfig}
\usepackage{graphicx}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Music Teaching Robot Platform for \\Children with Autism\\
{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
should not be used}
\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Huanghao Feng}
\IEEEauthorblockA{\textit{Ritchie School of Engineering \& Computer Science} \\
\textit{University of Denver}\\
Denver, USA \\
huanghao.feng@du.deu}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Mohammad H.Mahoor}
\IEEEauthorblockA{\textit{Ritchie School of Engineering \& Computer Science} \\
\textit{University of Denver}\\
Denver, USA \\
m.mahoor@du.edu}

}

\maketitle

\begin{abstract}

\end{abstract}

\begin{IEEEkeywords}
Social Robot, Autism, Music Therapy, Turn-Taking
\end{IEEEkeywords}


\section{Introduction}
Individuals with autism spectrum disorder experience verbal and nonverbal
communication impairments, including motor control, emotional facial expressions, and
eye gaze attention. Oftentimes, individuals with high-functioning autism have deficits in
different areas, such as (1) language delay, (2) difficulty in having empathy with their peer
and understanding others emotions (i.e. facial expressions recognition.), and more
remarkably (3) joint attention (i.e. eye contact and eye gaze attention). 
They might also have several other unusual social developmental
behaviors that may appear in infancy or childhood. For instance children with autism show
less attention to social stimuli (e.g. turn-taking),that is why it would be difficult for 
individuals with autism to maintain social interaction with others \cite{InfantileAutism1975}.\\

Turn-taking is a type of organization in conversation and discourse where participants speak 
one at a time in alternating turns. In practice, it involves processes for constructing 
contributions, responding to previous comments, and transitioning to a different speaker, 
using a variety of linguistic and non-linguistic cues.\cite{BehaviorlaStudy1964}
While the structure is generally universal,\cite{RoboticMovement} that is, overlapping talk is generally 
avoided and silence between turns is minimized, turn-taking conventions vary by culture 
and community.\cite{EnhanceEmpiri2011} Conventions vary in many ways, such as how turns are distributed, how 
transitions are signaled, or how long is the average gap between turns.
In many contexts, conversation turns are a valuable means to participate in social life 
and have been subject to competition.\cite{DOMER2011} It is often thought that turn-taking strategies 
differ by gender; consequently, turn-taking has been a topic of intense examination in 
gender studies. While early studies supported gendered stereotypes, such as men interrupting
more than women and women talking more than men,\cite{DefineSocial2005} recent research has found mixed evidence 
of gender-specific conversational strategies, and few overarching patterns have emerged.\cite{SocialInteract2003}\\
%https://en.wikipedia.org/wiki/Turn-taking

Motor control is the systematic regulation of movement in organisms that possess 
a nervous system. Motor control includes movement functions which can be attributed 
to reflex,\cite{BehaviorlaStudy1964}. Motor control as a field of study is primarily a sub-discipline of 
psychology or neurology. While the modern 
study of motor control is an increasingly interdisciplinary field, research 
questions have historically been defined as either physiological or psychological, 
depending on whether the focus is on physical and biological properties, or 
organizational and structural rules.\cite{DOMER2011} Areas of study related to motor control 
are motor coordination, motor learning, signal processing, and perceptual control 
theory.\\

Recent study shows that music has played a important role in children's daily life such as 
waking time, streaming from radios, televisions, cell-phones, computers and toys.\cite{Towards}
Since children with autism spent most of the time with technology product nowadays, music
could play an important role in their life. 

Music, performed by instrument, is created to express emotions. 
However, learning how to play can be a different story, even to those who have talent in music.
Facial expressions could be a reliable representation of emotions from individuals for most
of the cases. However, it could be challenge for specific populations such as Autism Spectrum
Disorders (ASDs), a grouping of disorders characterized by profound difficulties with social
interaction, have demonstrated impairments in facial emotion recognition. Indeed, Kanner (1943)
originally described autism as a 'disorder of affective contact', and the current DSM-IV-TR 
diagnostic criteria for ASDs include items related to deficits in identifying and processing 
emotions: "marked impairments in the use of multiple nonverbal behaviors, such as ...facial
expression..." and "lack of social or emotional reciprocity" (APA 2000). 
There are three different perspectives dealing with affect in multimedia,
namely, expressed emotions, felt emotions and expected emotions, 

% this part should talk about connection between turn-taking in autism and the connection
% between music and autism, then introduce the how turn taking can be used in music interaction
% in autism. music practice with a instructor can cause stress, that can lead the eda topic
% later.

Electrodemal activity (EDA) signals which is considered as a periphery 
Individuals with autism spectrum disorder experience verbal and nonverbal
communication impairments, including motor control, emotional facial expressions, and
eye gaze attention. Oftentimes, individuals with high-functioning autism have deficits in
different areas, such as (1) language delay, (2) difficulty in having empathy with their peer
and understanding others emotions (i.e. facial expressions recognition.), and more
remarkably (3) joint attention (i.e. eye contact and eye gaze attention). 
Although there is no single accepted intervention, treatment,
or known cure for ASDs, these individual will have more successful treatment if ASD is
diagnosed in early stages. People who suffer from autism might also have several other unusual social developmental
behaviors that may appear in infancy or childhood. For instance children with autism show
less attention to social stimuli (e.g. turn taking, motor control). 
Compared with typically developing children, older children or
adults with autism can read facial expressions less effectively and recognize emotions
behind specific facial expressions or the tone of voice with difficulties \cite{LogicScien1959}. In contrast to
TD individuals, children with autism (i.e. high-functioning, Asperger syndrome) may be
overwhelmed with social signals such as facial behaviors and expression and complexity
of them and they suffer from interacting with other individuals, therefore they would prefer
to be alone. That is why it would be difficult for individuals with autism to maintain social
interaction with others \cite{InfantileAutism1975}.\\

\section{Related Works}

\subsection{Autism}


%https://en.wikipedia.org/wiki/Motor_control

\subsection{Electrodermal activity (EDA) and Emotion Classification}
Emotion is an intense mental experience often manifested by rapid heartbeat, breathing, 
sweating, and facial expressions. Emotion recognition from these physiological signals 
is a challenging problem with interesting applications such as developing wearable 
assistive devices and smart human-computer interfaces. This paper presents an automated 
method for emotion classification in children using electrodermal activity (EDA) signals. 
The time-frequency analysis of the acquired raw EDAs provides a feature space based on 
which different emotions can be recognized. To this end, the complex Morlet (C-Morlet) 
wavelet function is applied on the recorded EDA signals. 

EDA has been used as an effective and reproducible electrophysiological method for 
investigating sympathetic nervous system function \cite{WearableDevice2016, 
	AssociationBetween2013, SympatheticSkin1984, PrincipalComponent2000}. Note that the sympathetic nervous 
burst changes the skin conductance, which can be traced by analyzing the EDA signals
\cite{SkinConduct2006, SympatheticSkin1981, DecodeChild2013}. The Q-sensor 
is a convenient wireless-based EDA device with no need for cables, boxes, or skin 
preparation. This device can track three types of data including EDA, temperature, 
and acceleration at the same time \cite{Validation2013}. It is worth mentioning that 
as of today, there has been no published work on emotion classification using the 
EDA signals collected by this dataset collected at the Georgia Institute of 
Technology \cite{DecodeChild2013}.\\

EDA signals are nonstationary and noisy; hence, wavelet-based analysis of EDA signals 
has been considered in the literature \cite{EmotionalState2013, EMGGSR2009}
either as a pre-processing step or a feature extraction approach for emotion classification. 
\cite{EmotionalState2013} used a set of wavelet coefficients representing EDA features 
together with heart rate signal to increase the percentage of correct classifications 
of emotional states and provide clearer relationships among the physiological response 
and arousal and valence. \cite{EDA2016} used a feature space based on the 
discrete wavelet transform (DWT) of the EDA signal to distinguish subjects suffering 
social anxiety disorder (SAD) and a control group. Using MLP and DWT features, they 
achieved a classification accuracy of ~85\%.
\\

Physiological responses have been identified as reliable indicators of human emotional 
and cognitive states. This section is dedicated to review some existing methods used for 
human emotion recognition based on various physiological responses, such as facial 
expression and other types of bio-signals. \\

A wearable glass device was designed by \cite{WearableDevice2016} to measure both electrodermal 
activity (EDA) and photoplethysmogram data for emotion recognition purposes. A built-in 
camera was also used in this device for capturing partial facial expression from the eye 
and nose area. This approach obtains remarkable performance in facial expression 
recognition in the subject-dependent cases. However, for subject-independent cases, 
it results in different accuracies across different types of emotions, which is an 
undesirable feature. \\

Several emotion classification methods have been presented in the literature using 
different bio-signals \cite{EmotionInten2014, EmotionResp2013, ElectAct2000, HeteroKnow2016}. 
Due to the variety of the signals used in these methods, 
different approaches have been designed to comply with their specific characteristics. 
Analysis of variance (ANOVA) and linear regression \cite{ElectAct2000} are the 
commonly used methods to extract features from bio-signals and to recognize different 
emotional states. These methods are based on the assumption of a linear relationship 
between the recorded signals and emotional states. A fuzzy-based classification 
method \cite{EmotionInten2014} has been used in to transform EDA and facial 
electromyography (EMG) to valence and arousal states. These states were then used 
to classify different emotions. \\

Support Vector Machine (SVM) is a well-known supervised learning algorithm that has 
extensively been used for pattern classification and regression \cite{SupportVector1995}. 
The SVM classifier tends to separate dataset by drawing an optimal hyperplane 
between classes such that the margin between them becomes maximum. The samples of 
each class that are located within the margin are called support vectors and play the 
main role in calculating the parameters of the hyperplanes between the corresponding 
classes. Machine learning algorithms such as SVM, linear discriminant analysis (LDA), 
and classification and regression tree (CART) have been employed for emotion 
classification purposes. For instance, in several works including \cite{Taxonomy2011, 
	EmotionClassifi2014}, the authors combined various types of bio-signals such as ECG, 
skin temperature (SKT), HR, and Photoplethysmogram (PPG) for  emotion classification 
purposes. \cite{FeatureSelection2006} proposed unsupervised clustering methods for emotion 
recognition. Their method benefited from several features obtained from different 
body responses such as SC, HR, and EMG. They showed that only a few statistical 
features such as the mean and standard deviation of the data can be relevant identifiers 
for defining different clusters. \\

To the best of our knowledge there are a few works \cite{EmotionResp2013, SlowEcho2009} 
that have studied and compared different automated 
classification techniques for emotion recognition of children with autism using EDA signals. 
This motivated us to conduct this study using an existing dataset, which concentrates 
on emotion classification of ASD children based on the relationship between their facial 
expressions and the collected EDA signals.

\section{Methodology}
Before you begin to format your paper, first write and save the content as a 
separate text file. Complete all content and organizational editing before 
formatting. Please note sections \ref{AA}--\ref{SCM} below for more information on 
proofreading, spelling and grammar.

Keep your text and graphic files separate until after the text has been 
formatted and styled. Do not number text heads---{\LaTeX} will do that 
for you.

\subsection{Xylo-Bot: An Interactive Music Teaching System}\label{AA}
A novel interactive human-robot music teaching system design is presented in 
this chapter. In order to make the robot play the xylophone properly, several things need 
to be done. First is to find a proper xylophone with correct timber; 
second, we have to arrange the xylophone in the proper position in front of the robot 
to make it visible and be reachable to play; finally, design the 
intelligent music system for NAO.\\

\subsection{NAO: A Humanoid Robot}
We used a humanoid  robot called NAO developed by Aldebaran Robotics in France. 
NAO is 58 cm (23 inches) tall, with 25 degrees of freedom. This robot 
can conduct most human behaviors. It also features an onboard multimedia 
system including four microphones for voice recognition and sound localization, 
two speakers for text-to-speech synthesis, and two HD cameras with maximum image 
resolution 1280 x 960 for online observation. As shown in Figure \ref{nao_body}, these 
utilities are located in the middle of the forehead and the mouth area. NAO’s 
computer vision module includes facial and shape recognition units. By using the 
vision feature of the robot, the robot can see the instrument 
from its lower camera and be able to implement an eye-arm self-calibration 
system which allows the robot to have real-time micro-adjustment of its 
arm-joints in case of off positioning during music playing.\\
\\

\begin{figure}[tbp]
	\begin{center}
		\begin{tabular}{c}
			\epsfig{figure=./fig/naobody.eps, scale = .4}\label{nao_body} \\
		\end{tabular}
		\caption{A Humanoid Robot NAO: 25 Degrees of Freedom, 2 HD Cameras and 4 Microphones} \label{nao_body}
	\end{center}
\end{figure}

The robot arms have a length of approximately 31 cm. Each arm has five degrees 
of freedom and is equipped with sensors to measure the position of each 
joint. To determine the pose of the instrument and the mallets' heads, the robot 
analyzes images from the lower monocular camera located in its head, which has a 
diagonal field of view of 73 degrees. These dimensions allow us to choose a 
proper instrument presented in the next section.\\
% show nao's open arms pic here, use the pic from german group
%\begin{figure}[tbp]
%	\begin{center}
%		\begin{tabular}{c}
%			\epsfig{figure=./chapters/fig/naobody.eps, scale = 0.45}\label{nao_open_arm} \\
%		\end{tabular}
%		\caption{NAO Open Arm with Playable Positions} \label{nao_open_arm}
%	\end{center}
%\end{figure}

The four microphone locations embedded on the toy or NAO's head can be seen in Figure \ref{nao_body}. 
According to the official Aldebaran documentation, these microphones have sensitivity 
of 20mV/Pa +/-3dB at 1kHz, and an input frequency range of 150Hz - 12kHz. Data 
will be recorded as a 16 bit, 48000Hz, 4 channels wav file which meets the 
requirements for designing the online feedback audio score system described below.\\
% show nao's head with mic details here


\subsection{Accessories}
The purpose of this study is to have a toy-size humanoid robot play music. Some necessary accessories 
needed to be purchased and made before the robot was able to play music. 
All accessories will be discussed in the following sections.\\

\subsection{Xylophone: A Toy for Music Beginner}
In this system, due to NAO's open arms' length, we choose a Sonor Toy Sound SM 
soprano-xylophone with 11 sound bars of 2 cm in width. The instrument has a size of 
31 cm x 9.5 cm x 4 cm, including the resonating body. The smallest sound bar is 
playable in an area of 2.8 cm x 2 cm, the largest in an area of 4.8 cm x 2 cm. The 
instrument is diatonically tuned in C-Major/a-minor. For the beaters/mallets, we used 
the pair that came with the xylophone with a modified 3D printed grip (details in next 
subsection) to allow the robot's hands to hold them properly. The mallets 
are approximately 21 cm in length and include a head of 0.8 cm radius.\\ 


The 11 bars of the xylophone represent 11 different notes (11 frequencies) which covers 
approximately a one and a half octave scale starting from C6 to F7. \\
%(see figures or a table with different frequencies somewhere)

\subsection{Mallet Gripper Design}
According to the size of Nao's hands, we designed and 3D printed a pair of gripers to 
have the robot be able to hold the mallets properly. All dimensions can be found 
in Figure \ref{griper}.\\

\begin{figure}[tbp]
	\begin{center}
		\begin{tabular}{c}
			\epsfig{figure=./fig/grip.eps, scale = 0.4}\label{griper} \\
		\end{tabular}
		\caption{Mallet Griper} \label{griper}
	\end{center}
\end{figure}

\subsection{Instrument Stand Design}
A wooden base was designed and laser cut to hold the instrument in the proper place 
for the robot to be able to play music. All dimensions can be found in 
Figure \ref{stand}.\\

\begin{figure}[tbp]
	\begin{center}
		\begin{tabular}{c}
			\epsfig{figure=./fig/front_view.eps, scale = 0.4} \label{front}\\
		\end{tabular}
		\caption{Instrument Stand Front View.} \label{stand}
	\end{center}
\end{figure}


\section{Module-Based Acoustic Music Interactive System Design}
In this section, a novel module-based robot-music teaching system will be presented. 
Three modules have been built in this intelligent system including module 1: eye-hand 
self-calibration micro-adjustment; module 2: joint trajectory generator; and 
module 3: real time performance scoring feedback. See Figure \ref{module}.\\

\begin{figure}[tbp]
	\begin{center}
		\begin{tabular}{c}
			\epsfig{figure=./fig/module_blocks.eps, scale = .3}\label{module} \\
		\end{tabular}
		\caption{Block Diagram of Module-Based Acustic Music Interactive System} \label{module}
	\end{center}
\end{figure}

\subsection{Module 1: Eye-hand Self-Calibration Micro-Adjustment}
Knowledge about the parameters of the robot's kinematic model is essential for 
tasks requiring high precision, such as playing the xylophone. While the kinematic 
structure is known from the construction plan, errors can occur, e.g., due to the 
imperfect manufacturing. After multiple rounds of testing, it was found the targeted angle chain 
of arms never actually equals the returned chain. We therefore used a 
calibration method to accurately eliminate these errors.\\


\subsubsection{Color-Based Object Tracking}
To play the xylophone, the robot has to be able to adjust its motions according to
the estimated relative position of the instrument and the heads of the beaters it is 
holding. To estimate these poses, adopted in this thesis, we 
used a color-based technique.\\
The main idea is, based on the RGB color of the center blue bar, given a hypothesis 
about the instrument's pose, one can project the contour of the object's model into the 
camera image and compare them to actually observed contour. In this way, it is possible 
to estimate the likelihood of the pose hypothesis. By using this method, it allows
the robot to track the instrument with very low cost in real-time. See Figure \ref{color_detection}\\
\begin{figure}[bp]
	\begin{center}
		\begin{tabular}{c}
			\epsfig{figure=./fig/color_detection.eps, scale = 0.35} \label{color_detection_c}\\

		\end{tabular}
		\caption{Color Detection From NAO's Bottom Camera Color Based Edge Detection.} \label{color_detection}
	\end{center}
\end{figure}


\subsection{Module 2: Joint Trajectory Generator}
Our system parses a list of hex-decimal numbers (from 1 to b) to obtain the sequence
of notes to play. It converts the notes into a joint trajectory using the beating
configurations obtained from inverse kinematics as control points. The timestamps
for the control points will be defined by the user in order to meet the experiment requirement.
The trajectory is then computed using Bezier interpolation in joint space by the
manufacturer-provided API and then sent to the robot controller for execution. In this
way, the robot plays in-time with the song.\\

\subsection{Module 3: Real-Time Performance Scoring Feedback}
The purpose of this system is to provide a real-life interaction experience using 
music therapy to teach kids social skills and music knowledge.  In this scoring 
system, two core features were designed to complete the task: 1) music detection;
2) intelligent scoring-feedback system.\\


\subsubsection{A. Music Detection}
Music, in the understanding of science and technology, can be considered as a combination 
of time and frequency. In order to make the robot detect a sequence of frequencies, we adopted the 
short-time Fourier transform (STFT) to this audio feedback system. This allows the robot to 
be able to understand the music played by users and provide the proper feedback as
a music teaching instructor.\\

The short-time Fourier transform (STFT) , is a Fourier-related transform used to 
determine the sinusoidal frequency and phase content of local sections of a signal 
as it changes over time. In practice, the procedure for computing STFTs is to divide 
a longer time signal into shorter segments of equal length and then compute the 
Fourier transform separately on each shorter segment. This reveals the Fourier 
spectrum on each shorter segment. One then usually plots the changing spectra as 
a function of time. In the discrete time case, the data to be transformed could 
be broken up into chunks or frames (which usually overlap each other, to reduce 
artifacts at the boundary). Each chunk is Fourier transformed, and the complex 
result is added to a matrix, which records magnitude and phase for each point in 
time and frequency. This can be expressed as:
\\

${\displaystyle \mathbf\{x[n]\}(m,\omega )\equiv X(m,\omega )=\sum _{n=-\infty }^{\infty }x[n]w[n-m]e^{-j\omega n}}$
\\

likewise, with signal x[n] and window w[n]. In this case, m is discrete and $\omega$ 
is continuous, but in most typical applications, the STFT is performed on a computer 
using the Fast Fourier Transform, so both variables are discrete and quantized.\\
The magnitude squared of the STFT yields the spectrogram representation of the Power 
Spectral Density of the function:
\\

${\displaystyle \operatorname {spectrogram} \{x(t)\}(\tau ,\omega )\equiv |X(\tau ,\omega )|^{2}}$\\

After the robot detects the notes from user input, a list of hex-decimal number will be
returned. This list will be used in two purposes: 1) to compare with the target list
for scoring and sending feedback to user; 2) used as a new input to have
robot playback in the game session as discussed in the next chapter.\\

\begin{figure}[tbp]
	\begin{center}
		\begin{tabular}{c}
			\epsfig{figure=./fig/stft.eps, scale = 1}\label{stft} \\
		\end{tabular}
		\caption{Melody Detection with Short Time Fourier Transform} \label{stft}
	\end{center}
\end{figure}

\subsubsection{B. Intelligent Scoring-Feedback System}
In order to compare the detected notes and the target notes, we used an algorithm
which is normally used in information theory linguistics called Levenshtein Distance.
This algorithm is a string metric for measuring the difference between two sequences.\\

In our case, the Levenshtein distance between two string-like hex-decimal numbers 
${\displaystyle a,b}$ (of length ${\displaystyle |a|}$ and ${\displaystyle |b|}$ respectively) 
is given by ${\displaystyle \operatorname {lev} _{a,b}(|a|,|b|)}$ where
\\

${\displaystyle \operatorname {lev} _{a,b}(i,j)={\begin{cases}\max(i,j)\\\min {\begin{cases}\operatorname {lev} _{a,b}(i-1,j)+1\\\operatorname {lev} _{a,b}(i,j-1)+1\\\operatorname {lev} _{a,b}(i-1,j-1)+1_{(a_{i}\neq b_{j})}\end{cases}}\end{cases}}}$\\
\\

where ${\displaystyle 1_{(a_{i}\neq b_{j})}}$ is the indicator function equal to 0 when 
${\displaystyle a_{i}=b_{j}}$ and equal to 1 otherwise, and ${\displaystyle \operatorname {lev} _{a,b}(i,j)}$ 
is the distance between the first ${\displaystyle i}$ characters of ${\displaystyle a}$ and the
first ${\displaystyle j}$ characters of ${\displaystyle b}$.

Note that the first element in the minimum corresponds to deletion (from ${\displaystyle a}$ to 
${\displaystyle b}$), the second to insertion and the third to match or mismatch, depending on 
whether the respective symbols are the same. Table \ref{LD} demonstrates how to apply this
principle in finding the Levenshtein distance of two words "Sunday" and "Saturday".\\

Based on the real life situation, we defined a likelihood margin for determining whether the result
is good or bad: \\

${likelihood = \dfrac{len(target) - lev_{target,source}}{len(target)}}$\\

where if the likelihood is greater than 66\% ~ 72\%, the system will consider it as a good result.
This result will be passed to the accuracy calculation system to have the robot decide whether it
needs to add more dosage to the practice. More details will be discussed in the next chapters
as it relates to the experiment design.\\

%\subsection{Units}
%\begin{itemize}
%\item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as ``3.5-inch disk drive''.
%\item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
%\item Do not mix complete spellings and abbreviations of units: ``Wb/m\textsuperscript{2}'' or ``webers per square meter'', not ``webers/m\textsuperscript{2}''. Spell out units when they appear in text: ``. . . a few henries'', not ``. . . a few H''.
%\item Use a zero before decimal points: ``0.25'', not ``.25''. Use ``cm\textsuperscript{3}'', not ``cc''.)
%\end{itemize}
%
%\subsection{Equations}
%
%\begin{equation}
%a+b=\gamma\label{eq}
%\end{equation}
%
%
%\subsection{\LaTeX-Specific Advice}
%
%Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
%of ``hard'' references (e.g., \verb|(1)|). That will make it possible
%to combine sections, add equations, or change the order of figures or
%citations without having to go through the file line by line.
%
%Please don't use the \verb|{eqnarray}| equation environment. Use
%\verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
%environment leaves unsightly spaces around relation symbols.
%
%Please note that the \verb|{subequations}| environment in {\LaTeX}
%will increment the main equation counter even when there are no
%equation numbers displayed. If you forget that, you might write an
%article in which the equation numbers skip from (17) to (20), causing
%the copy editors to wonder if you've discovered a new method of
%counting.
%
%{\BibTeX} does not work by magic. It doesn't get the bibliographic
%data from thin air but from .bib files. If you use {\BibTeX} to produce a
%bibliography you must send the .bib files. 
%
%{\LaTeX} can't read your mind. If you assign the same label to a
%subsubsection and a table, you might find that Table I has been cross
%referenced as Table IV-B3. 
%
%{\LaTeX} does not have precognitive abilities. If you put a
%\verb|\label| command before the command that updates the counter it's
%supposed to be using, the label will pick up the last counter to be
%cross referenced instead. In particular, a \verb|\label| command
%should not go before the caption of a figure or a table.
%
%Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
%will not stop equation numbers inside \verb|{array}| (there won't be
%any anyway) and it might stop a wanted equation number in the
%surrounding equation.
%
%\subsection{Some Common Mistakes}\label{SCM}
%\begin{itemize}
%\item The word ``data'' is plural, not singular.
%\item The subscript for the permeability of vacuum $\mu_{0}$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
%\item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
%\item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
%\item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
%\item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
%\item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
%\item Do not confuse ``imply'' and ``infer''.
%\item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
%\item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
%\item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.
%\end{itemize}
%An excellent style manual for science writers is \cite{b7}.
%
%\subsection{Authors and Affiliations}
%\textbf{The class file is designed for, but not limited to, six authors.} A 
%minimum of one author is required for all conference articles. Author names 
%should be listed starting from left to right and then moving down to the 
%next line. This is the author sequence that will be used in future citations 
%and by indexing services. Names should not be listed in columns nor group by 
%affiliation. Please keep your affiliations as succinct as possible (for 
%example, do not differentiate among departments of the same organization).
%
%\subsection{Identify the Headings}
%Headings, or heads, are organizational devices that guide the reader through 
%your paper. There are two types: component heads and text heads.
%
%Component heads identify the different components of your paper and are not 
%topically subordinate to each other. Examples include Acknowledgments and 
%References and, for these, the correct style to use is ``Heading 5''. Use 
%``figure caption'' for your Figure captions, and ``table head'' for your 
%table title. Run-in heads, such as ``Abstract'', will require you to apply a 
%style (in this case, italic) in addition to the style provided by the drop 
%down menu to differentiate the head from the text.
%
%Text heads organize the topics on a relational, hierarchical basis. For 
%example, the paper title is the primary text head because all subsequent 
%material relates and elaborates on this one topic. If there are two or more 
%sub-topics, the next level head (uppercase Roman numerals) should be used 
%and, conversely, if there are not at least two sub-topics, then no subheads 
%should be introduced.

\subsection{Figures and Tables}
\paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
bottom of columns. Avoid placing them in the middle of columns. Large 
figures and tables may span across both columns. Figure captions should be 
below the figures; table heads should appear above the tables. Insert 
figures and tables after they are cited in the text. Use the abbreviation 
``Fig.~\ref{fig}'', even at the beginning of a sentence.

\begin{table}[htbp]
\caption{Table Type Styles}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
\cline{2-4} 
\textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
\hline
copy& More table copy$^{\mathrm{a}}$& &  \\
\hline
\multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
\end{tabular}
\label{tab1}
\end{center}
\end{table}

\begin{figure}[htbp]
\centerline{\includegraphics{fig1.png}}
\caption{Example of a figure caption.}
\label{fig}
\end{figure}

Figure Labels: Use 8 point Times New Roman for Figure labels. Use words 
rather than symbols or abbreviations when writing Figure axis labels to 
avoid confusing the reader. As an example, write the quantity 
``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
units in the label, present them within parentheses. Do not label axes only 
with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
\{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
quantities and units. For example, write ``Temperature (K)'', not 
``Temperature/K''.

\section*{Acknowledgment}

The preferred spelling of the word ``acknowledgment'' in America is without 
an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
acknowledgments in the unnumbered footnote on the first page.

%\section*{References}

\baselineskip 0.21in
\bibliographystyle{plain}
\bibliography{ref}

%\begin{thebibliography}{00}
%\bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
%\bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
%\bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
%\bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
%\bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
%\bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
%\bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
%\end{thebibliography}


\end{document}
