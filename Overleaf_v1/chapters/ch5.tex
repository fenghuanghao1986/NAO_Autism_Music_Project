\chapter{Results of Social Behavior Intervention and Emotion Classification}
In this chapter several questions will be addressed:
1) Turn-taking: How well do children with ASD behave in the music activities?\\
2) Motor control: How well do children with ASD play xylophone in terms of volume, pitch, and accuracy
after the intervention sessions? e.g., excellent multiple strikes should be recognized by 
STFT as a sequence of frequencies;\\
3) Social Engagement: How did children engage with different level of music teaching events?\\
4) Emotion Fluctuation: How did emotion change among different activities? How did emotion change in 
a single event? What is the difference between the target and control groups?\\

\section{Social Behavior Result}
\subsection{Motor Control}
Nine ASD and 7 TD participants finished this study in 8 months. All the ASD subjects completed 
6 sessions including the intervention sessions and the TDs participated in only 2 sessions (a baseline and an exit session). By using a Wizard of Oz control style, a well-trained researcher conducted the baseline and exit sessions. %for better observation
%and evaluation quality of performance
With the well designed and fully automated intervention sessions, NAO was
able to initiate music teaching activities with the participants. 

Since the music detection method was sensitive to the audio input, that required clear and long-lasting 
sound from a xylophone. From Figure \ref{warmup}, it was evident that the majority of subjects were able 
to strike or play xylophone properly after one or two sessions. Notice that subject 101 and 
102 had a significant improvement curve during the intervention sessions. Some of the subjects started 
at a higher accuracy rate and kept this rate above 80\%, which can be considered as consistent 
motor control performance even with some ups and downs. Two subjects (103 \& 107) had a difficult 
time playing the xylophone and following the turn-taking cues with the robot. This fact affected 
the performance in the following activities for both subjects.\\

\begin{figure}[tbp]
	\begin{center}
		\begin{tabular}{c}
			\epsfig{figure=./chapters/fig/warm.eps, scale = 0.9}\label{warmup} \\
		\end{tabular}
		\caption{Motor Control Accuracy Result} \label{warmup}
	\end{center}
\end{figure}

Figure \ref{song} shows the accuracy of the leading music teaching activity for intervention sessions
across all the participants. Learning how to play one's favorite song can be considered as a motivation for ASD
kids understanding and learning turn-taking skills. As described in the previous section, the difficulty 
level of this activity was designed uprising. By this fact, the accuracy of the performance from participants 
was expected to decrease or maintain at the same level. This activity required participants to concentrate and utilize joint attention skills in the robot teaching stage and also respond properly afterward. Enough waiting time was given after the robot
says: 'Now, you shall play right after my eye flashes,' participants have also received an eye color change
cue from the robot in order to complete a desired music-based social interaction. Different from the warm
up section, notes played in the correct sequence of order can be considered as a good-count strike.
From Figure \ref{song}, we can say that most of the participants were able to complete single/multiple notes practices with an
average accuracy rate of 77.36\%/69.38\%, although even with color hints, the notes' pitch difference still can be a significant 
challenge for them. Due to the difficulties of sessions 4 and 5, a lower performance compared to the previous 
two sessions was acceptable. However, more than half of the participants showed consistently an acceptable performance
(i.e., accuracy) or even better results than previous sessions. Combining the report from video annotators, 6 out 
of 9 subjects showed stable and engaging behavior in playing music, especially after the first few sessions. Better 
learn-and-play turn-taking rotation was performed over time and a significant increase in performance by 3 subjects, 
reveal that the turn-taking skills were picked up from this activity.

\begin{figure}[tbp]
	\begin{center}
		\begin{tabular}{c}
			\epsfig{figure=./chapters/fig/song.eps, scale = 0.9}\label{song} \\
		\end{tabular}
		\caption{Main Music Teaching Performance Accuracy} \label{song}
	\end{center}
\end{figure}

\subsection{Turn-Taking Behavior}
Measuring turn-taking behavior can be subjective. In the current study, a grading system were designed in
order to have convincing result. Music teaching activity can be considered as "conversation" between 
instructor and student. 4 different behaviors are defined in the grading system: (a) "well-done", this
level is consider as a good behavior, participant should be able to finish listen to the instruction from
NAO, start playback after receive the command, and wait for the result without interrupting, 3 points for 
each "well-done"; (b) "lite-interrupt", in this level, participant may show slightly impatient in different
stages for example did not wait for the proper moment to play or did not pay attention to the result, 2 points
will be given in this level; (c) "heavy-interrupt", more interruptions may accrue in this level, participant
may interrupt the conversation at anytime but still willing to playback to the robot, 1 point for this level;
(d) "indifferent", participant shows less interest in music activity including but not limited to following 
behaviors, not willing to play, not listen to robot or play irrelevant music in one conversation, this level
will score a 0 point. The higher the score the better turn-taking behavior the participant has. All scores 
are normalized into percentage due to the difference of total "conversation" numbers. Figure \ref{turn} shows 
the total result among all subjects. Note that, 6 out of 9 participants can perform consistent turn-taking behavior 
during intervention sessions. \\

\begin{figure}[tbp]
	\begin{center}
		\begin{tabular}{c}
			\epsfig{figure=./chapters/fig/turn.eps, scale = 1.5}\label{turn} \\
		\end{tabular}
		\caption{Normalized Turn-taking behavior result for all subjects during intervention sessions. 
		} \label{turn}
	\end{center}
\end{figure}

\section{Music Emotion Classification Result}
Since we have developed our emotion classification method based on the time-frequency analysis of the
EDA signals, we first present the main properties of the continuous wavelet transform assuming complex Morlet
wavelet. Then, we discuss the pre-processing steps as well as the wavelet-based
feature extraction scheme. Finally, we briefly review the characteristics of the
support vector machine as the classifier used in our approach.\\

EDA signal was collected in this study. By using the annotation and analysis method from our
pre-study \cite{feng2018wavelet}, a music-event-based emotion classification result will 
be presented below. In order to find out the emotional experiences of the ASD group, multiple comparisons
were made after annotating the videos. Different activities may cause emotional arousal change. 
As presented above, the warm up section and single activity practice section have the same activity in 
different levels of intensities, and gameplay has the lowest difficulty and more relax as designed in purpose. 

The annotation was administrated supported the temporal relation between the video frames and 
the recorded EDA sequences of every subject. In different words, the annotator went through 
the entire video file of every event frame by frame, and designated the frames regarding 
the initiation finish of an emotion. Meanwhile, the corresponding sequences of 
the EDA signals were hold on to come up with the dataset for every perceived emotion. 
The music activities were designed to stimulate different emotions: (S1): "warm-up",
(S2): "music practice", and (S3): "music game". During the first part annotation, it is not obvious to
conclude the facial expression changes from different activities, however, in S1 participants 
showed more "calm" for most of the time due to the simplicity of completing the task;
in S2, the annotator could not have precise conclusion by reading the facial expression from 
participants as well, most of the subjects intent to play music and complete the task, "frustrated" 
can be the best to describe them; a very similar feeling can be found in S3 comparing to S1, in 
S3, all activities were designed to create a role changing environment for users in order to 
stimulate a different emotion, most of kids showed "happy" during the music game section.
Figure \ref{eda_anno} shows the above-described procedure diagrammatically. Due the fact
that it is hard to conclude these emotions with specific facial expressions, event numbers will be 
used in following analysis representing emotion comparison. Note that there 
is total of 21\% on average of emotions are not clear during the first annotation stage. 
It is necessary to have them as unclear rather than label them with specific categories. \\

\begin{figure}[tbp]
	\begin{center}
		\begin{tabular}{c}
			\epsfig{figure=./chapters/fig/eda_anno.eps, scale = 1.6}\label{eda_anno} \\
		\end{tabular}
		\caption{The distribution of the targeted emotions across all subjects and events. 
		} \label{eda_anno}
	\end{center}
\end{figure}

The continuous wavelet transform (CWT) of the data assuming complex Morlet (C-Morlet) wavelet function
was used inside a frequency range of (0.5, 50)Hz, and an SVM classifier was then employed to classify
"conversation" segmentation among 3 sections using the wavelet-based features. Table \ref{tab1}
shows the classification accuracy for the SVM classifier with different kernel functions. 
As can be seen, emotion difference between warm-up (S1) and music practice (S2), S2 and music game (S3) can be classified using wavelet-based feature extraction SVM classifier with an average accuracy of 76\% and 70\%. With the highest 64\% of accuracy
for S1 and S3, that may indicate fewer emotion changes between the warm-up and game sections. \\

\begin{table*}[tbp]
	\label{tab1}
	\begin{center}
	\caption{Emotion change in different events using wavelet-based feature extraction under SVM classifier. }
	\vspace{3mm}
		\begin{tabular}{llllll}
			& Kernels                     & Accuracy & AUC & Precision & Recall \\
			\hline
			S1 vs S2       & \multirow{4}{*}{\textbf{Linear}}     & 75       & 78  & 76        & 72     \\
			S1 vs S3       &                             & 57       & 59  & 56        & 69     \\
			S2 vs S3       &                             & 69       & 72  & 64        & 86     \\
			S1 vs S2 vs S3 &                             & \multicolumn{4}{l}{52}              \\
			\hline
			S1 vs S2       & \multirow{4}{*}{\textbf{Polynomial}} & 66       & 70  & 70        & 54     \\
			S1 vs S3       &                             & 64       & 66  & 62        & 68     \\
			S2 vs S3       &                             & 65       & 68  & 62        & 79     \\
			S1 vs S2 vs S3 &                             & \multicolumn{4}{l}{50}              \\
			\hline
			S1 vs S2       & \multirow{4}{*}{\textbf{RBF}}        & 76       & 81  & 76        & 75     \\
			S1 vs S3       &                             & 57       & 62  & 57        & 69     \\
			S2 vs S3       &                             & 70       & 76  & 66        & 83     \\
			S1 vs S2 vs S3 &                             & \multicolumn{4}{l}{53}              \\
			\hline
		\end{tabular}
		\label{tab1}
	\end{center}
\end{table*}

In the second part of the analysis, EDA signals were segmented into small event-based pieces according to 
the number of "conversations" in each section as mention before. One "conversation" was defined with 3 segments:
a) robot/participant demonstrates the note(s) to play; b) participant/robot repeat the note(s); 
c) robot/participant presents the result, and each segmentation lasts about 45 seconds.
In order to discover the emotion fluctuation inside one task, each "conversation" section
has been carefully divided into 3 segments, as described before. Each segment lasts about 10 - 20 seconds.
Table \ref{tab2} shows the full result of
emotion fluctuation in the warm-up (S1) and music practice (S2) sections from the intervention session. Notice that
all of the segments cannot be appropriately classified using the existing method. Both SVM and KNN show
stable results. This may suggest that the ASD group may have less emotion fluctuation or arousal
change once the task starts even with various activities in it. Stable emotion arousal in a single task
could also benefit from the proper activity content, including robot agents play music and language
used during the conversation. Friendly voice feedback was based on the performance delivered to participants
who were well written and stored in memory, both favorable awards while receiving correct input and encouragement
while play incorrectly. Since emotion fluctuation can affect learning progress, less arousal change 
indicates the design of intervention sessions are robust. 

\begin{sidewaystable}[tbp]
	\label{tab2}
	\begin{center}
	\caption{Emotion change classification performance in single event with segmentation using both SVM and KNN classifier. }
	\vspace{3mm}
		\resizebox{\columnwidth}{!}{
		\begin{tabular}{ccccccccc}
			\multicolumn{1}{l}{\multirow{3}{*}{}} & \multicolumn{5}{c}{Segmentation Comparison in Single Task}                                                                                                       \\
			\hline
			\multicolumn{1}{l}{}                  & \multicolumn{4}{c}{Warm up Section}                                                   & \multicolumn{4}{c}{Song Practice Section}                                                  \\
			\hline
			\multicolumn{1}{l}{}                  & Kernels                     & Accuracy & K value                & Accuracy & Kernels                     & Accuracy & K value                & Accuracy \\
			\hline
			learn vs play                                   & \multirow{4}{*}{Linar}      & 52.62    & \multirow{4}{*}{K = 1} & 54       & \multirow{4}{*}{Linar}      & 53.79    & \multirow{4}{*}{K = 1} & 52.41    \\
			learn vs feedback                                   &                             & 53.38    &                        & 50.13    &                             & 53.1     &                        & 51.72    \\
			play vs feedback                                   &                             & 47.5     &                        & 50.38    &                             & 54.31    &                        & 50.86    \\
			learn vs play vs feedback                                 &                             & 35.08    &                        & 36.25    &                             & 35.52    &                        & 36.55    \\
			\hline
			learn vs play                                   & \multirow{4}{*}{Polynomial} & 49       & \multirow{4}{*}{K = 3} & 50.25    & \multirow{4}{*}{Polynomial} & 53.79    & \multirow{4}{*}{K = 3} & 50.69    \\
			learn vs feedback                                 &                             & 50.75    &                        & 50.13    &                             & 50.86    &                        & 50.34    \\
			play vs feedback                                   &                             & 49.87    &                        & 49.5     &                             & 49.14    &                        & 52.07    \\
			learn vs play vs feedback                                 &                             & 33.92    &                        & 35.83    &                             & 34.71    &                        & 35.29    \\
			\hline
			learn vs play                                   & \multirow{4}{*}{RBF}        & 54.38    & \multirow{4}{*}{K = 5} & 48.37    & \multirow{4}{*}{RBF}        & 50.86    & \multirow{4}{*}{K = 5} & 50.17    \\
			learn vs feedback                                   &                             & 55.75    &                        & 52.75    &                             & 53.97    &                        & 50.17    \\
			play vs feedback                                   &                             & 51.12    &                        & 50       &                             & 53.79    &                        & 52.93    \\
			learn vs play vs feedback                                 &                             & 36.83    &                        & 34.17    &                             & 34.83    &                        & 33.1    \\
			\hline
		\end{tabular}

	}
		\label{tab2}
	\end{center}
\end{sidewaystable}

Cross-section comparison is also presented blow. Since each "conversation" contains 3 segments, it is 
necessary to have specific segments from one task to compare with the other task corresponded to. 
Table \ref{tab3} shows the classification rate in robot demo, kids play, and robot feedback across
warm-up (S1) and music practice (S2) sections. By using RBF kernel, wavelet-based SVM classification rate has
~80\% of accuracy for all 3 comparisons. This result also matches the result from Table \ref{tab1}. 

\begin{table*}[tbp]
	
	\begin{center}
	\caption{classification rate in robot demo, kids play and robot feedback across
			warm up (S1) and music practice (S2) sections.}
			\vspace{3mm}
		\label{tab3}
		\begin{tabular}{lcccccc}
			\multicolumn{1}{c}{\multirow{2}{*}{}} & \multicolumn{3}{c}{Accuracy of SVM} & \multicolumn{3}{c}{Accuracy of KNN} \\
			\hline
			\multicolumn{1}{c}{}                  & Linear   & Polynomial    & RBF    & K = 1   & K = 3     & K = 5     \\
			\hline
			learn 1 vs learn 2                                 & 73.45    & 69.31   & 80.86  & 73.28   & 71.03   & 65      \\
			\hline
			play 1 vs play 2                                 & 75.34    & 68.79   & 80     & 74.48   & 69.14   & 64.31   \\
			\hline
			feedback 1 vs feedback 2                                 & 76.38    & 69.48   & 80.34  & 74.14   & 69.14   & 66.9   
		\end{tabular}
		 
		\label{tab3}
	\end{center}
\end{table*}


The types of activities and processes of the session between the baseline session for both groups were 
the same. By using the "conversation" concept above, each of them has been segmented.
Comparing with target and control groups using the same classifier, 80\% of accuracy for detecting
different groups. See Table \ref{tab3}. Video annotators also reported "unclear" in reading 
facial expressions from the ASD group. These combined messages suggest that even with the same activities,
different bio-reaction was completely opposite between TD and ASD groups. It has 
also been reported that significant improvement of music performance was shown in the ASD group, 
although both groups have a similar performance at their baseline sessions. Furthermore, the TD group 
were shown more willing to try to make their performance as better as possible while they made
mistakes.\\

\begin{table*}[tbp]
	\label{tab4}
	
	\begin{center}
	\caption{TD vs ASD Emotion Changes from Baseline and Exit Sessions.}
	\vspace{3mm}
		\begin{tabular}{llllll}
			& \textbf{Linear} & \textbf{Polynomial} & \textbf{RBF} \\
			\hline
			
			\textbf{Accuracy}                          & 75              & 62.5                & 80           \\
			\hline
			\multirow{2}{*}{\textbf{Confusion Matrix}} & 63  37          & 50  50              & 81  19       \\
			& 12  88          & 25  75              & 25  75       \\
			\hline
		\end{tabular}
		\label{tab4}
	\end{center}
\end{table*}

\section{Summary}
All the experimental results are presented in this chapter; answers to the questions mentioned in the beginning of the chapter can be found out from them. According to the report from the annotator, most of the kids (both ASD and TD groups) showed
well turn-taking communication behavior among all sessions. However, differences can also be found
when comparing both groups. All TD participants could initiate the activities from the beginning of the
session, while some of the ASD kids needed some help, although, after several visits (i.e., intervention
sessions), most of them could perform the turn-taking skills well. In terms of motor 
control skills, as can be seen from Figure \ref{warmup}, most of the ASD participants
can master this skill after the first few visits. For the ones who may not play xylophone properly, an improvement
also can be observed in this figure. Based on the recorded videos and Figure \ref{song}, 
more than half of the ASD kids well engaged during the intervention sessions. 
Few of them needed help from the researcher to complete the tasks in the first
or second sessions. Since each individual chose their favorite music, this could provide a certain level of motivation which is more engaging even with the repetitive activities. 
From Chapter 3, we learned that emotion classification using the EDA signal could be possible, and a wavelet-based feature extraction method was developed and applied in a similar research with a group of younger children. In this chapter, we adopted the proposed approach in order to decode the emotion fluctuation with music social activities for children with autism.
Multiple experiments were established in this chapter and emotion change was compared across different
events, within one activity and between the target and control group. 
It can be found that, from Table \ref{tab1} different titled music activities can stimulate different emotion changes.
Warm-up as a single note play activity without pitch correctness which makes it the most easiest
activity compare to the music practice activities. To this fact, less stress can be caused in the 
warm-up section during intervention sessions. Similar to the music game section, participants were not
require to consider how well they play but only challenge the robot to mimic what they have played. 
This can explain the reason why S2 can be classified from S1 and S3. Intra-activity emotion are also
discussed in this chapter see Table \ref{tab2}. By comparing different segments of one activity, it is hard to tell the emotion
changes between segments in one "conversation" among all activities. At some point, emotion in learning 
how to play xylophone may have less difference between playback to the instructor of what have just learned.
This may suggests that well social behaviors may be benefit from less emotion fluctuation in music activities
combining the results from Figure \ref{warmup} and Figure \ref{song}. More further discussion and conclusion 
can be found in the last chapter.
